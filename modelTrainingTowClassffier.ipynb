{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fbcad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "rand_seed = 0  # random state for reproducibility\n",
    "np.random.seed(rand_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('data/datasix.xlsx')\n",
    "data = data.dropna()\n",
    "data.head()\n",
    "data.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dca9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['label'] != 'محايد']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad8f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "pP=data[data['label'] == 'ايجابي']\n",
    "pNg=data[data['label'] == 'سلبي']\n",
    "print(len(data))\n",
    "print(len(pP),len(pNg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fd8fc6",
   "metadata": {},
   "source": [
    "# posative negative on comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca4330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM SPLIT\n",
    "\n",
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "#LABE4L FEATURE\n",
    "\n",
    "\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "  # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, test_data = random_split(data, features, output, train_fraction, rand_seed)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "\n",
    "print(len(train_data)+len(test_data))\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testNE=test_data[test_data['label'] == 'سلبي']\n",
    "testNg=test_data[test_data['label'] == 'ايجابي']\n",
    "\n",
    "print(len(test_data))\n",
    "print(len(testNE),len(testNg))\n",
    "print(\"_______________\")\n",
    "\n",
    "\n",
    "\n",
    "trainNE=train_data[train_data['label'] == 'سلبي']\n",
    "trainNg=train_data[train_data['label'] == 'ايجابي']\n",
    "print(len(train_data))\n",
    "print(len(trainNE),len(trainNg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b8213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainNE=train_data[train_data['label'] == 'سلبي']\n",
    "trainNg=train_data[train_data['label'] == 'ايجابي']\n",
    "print(len(train_data))\n",
    "print(len(trainNE),len(trainNg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TF IDF\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True, max_df=0.5, stop_words=None, use_idf=True)\n",
    "train_data_features = vectorizer.fit_transform(train_data['comment'])\n",
    "test_data_features = vectorizer.transform(test_data['comment'])\n",
    "\n",
    "#SHAPE\n",
    "\n",
    "train_data_features.shape, test_data_features.shape\n",
    "\n",
    "#FUNCTION FOR MODEL TRAIN\n",
    "\n",
    "\n",
    "\n",
    "#SHAPE\n",
    "\n",
    "train_data_features.shape, test_data_features.shape\n",
    "\n",
    "#FUNCTION FOR MODEL TRAIN\n",
    "\n",
    "\n",
    "def train_n_test_classifier(clf, train_features, train_labels, test_features, test_labels,data):\n",
    "    clf.fit(train_features, train_labels) # please learn patterns from the data\n",
    "\n",
    "   \n",
    "    print('-'*100+str(clf)[0:15])\n",
    "    print(\"accuracy_score Score on training data:\")\n",
    "    print(clf.score(train_features, train_labels))\n",
    "    \n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "    print(\"score on testing data:\")\n",
    "    \n",
    "    pred_y = clf.predict(test_features)\n",
    "    \n",
    "    data['predict']=pred_y\n",
    " \n",
    "    count=0\n",
    "    \n",
    "    print(\"accuracy_score Score on test data:\")\n",
    "    print(accuracy_score(test_labels, pred_y))\n",
    "    \n",
    "    print(\"f1_score  on test data:\")\n",
    "    print(f1_score(test_labels, pred_y, average='macro'))\n",
    "    filename='result_'+str(clf)[0:15]+'.xlsx'\n",
    "    data.to_excel(filename)\n",
    "    \n",
    "    \n",
    "    sentiment_fit=clf.fit(train_features,train_labels)\n",
    "    y_pred=sentiment_fit.predict(test_features)\n",
    "    print(classification_report(test_labels,y_pred,target_names=('posative','negative')))\n",
    "    \n",
    "    \n",
    "\n",
    "#LOGASTIC REGRESSION\n",
    "\n",
    "\n",
    "\n",
    "logistic_reg = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(logistic_reg, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "\n",
    "# MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "train_n_test_classifier(mnb,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "# S V M\n",
    "\n",
    "svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(svm,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "# MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(mlp,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "# KNN\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "train_n_test_classifier(knn, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "#TREE\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "train_n_test_classifier(dtree, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "estimator = []\n",
    "\n",
    "\n",
    "estimator.append(('LR', LogisticRegression(solver ='lbfgs',  multi_class ='multinomial',  max_iter = 200)))\n",
    "estimator.append(('SVC', SVC(gamma ='auto', probability = True)))\n",
    "estimator.append(('DTC', DecisionTreeClassifier()))\n",
    "estimator.append(('mnb',MultinomialNB()))\n",
    "estimator.append(('mlp',MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)))\n",
    "estimator.append(('knn',KNeighborsClassifier(n_neighbors=3)))     \n",
    "\n",
    "\n",
    "vot_hard = VotingClassifier(estimators = estimator, voting ='hard')\n",
    "\n",
    "train_n_test_classifier(vot_hard, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "vot_soft = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "\n",
    "train_n_test_classifier(vot_soft, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "Gradientclf = GradientBoostingClassifier(n_estimators=100,learning_rate=1.0,max_depth=1, random_state=0)\n",
    "\n",
    "train_n_test_classifier(Gradientclf,train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "adaClf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "train_n_test_classifier(adaClf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(rf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6daed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM SPLIT\n",
    "\n",
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "#LABE4L FEATURE\n",
    "\n",
    "\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "  # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, test_data = random_split(data, features, output, train_fraction, rand_seed)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "\n",
    "print(len(train_data)+len(test_data))\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "# BOW\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "train_data_features = vectorizer.fit_transform(train_data['comment'])\n",
    "\n",
    "test_data_features = vectorizer.transform(test_data['comment'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#SHAPE\n",
    "\n",
    "train_data_features.shape, test_data_features.shape\n",
    "\n",
    "#FUNCTION FOR MODEL TRAIN\n",
    "\n",
    "\n",
    "def train_n_test_classifier(clf, train_features, train_labels, test_features, test_labels,data):\n",
    "    clf.fit(train_features, train_labels) # please learn patterns from the data\n",
    "\n",
    "   \n",
    "    print('-'*100+str(clf)[0:15])\n",
    "    print(\"accuracy_score Score on training data:\")\n",
    "    print(clf.score(train_features, train_labels))\n",
    "    \n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "    print(\"score on testing data:\")\n",
    "    \n",
    "    pred_y = clf.predict(test_features)\n",
    "    \n",
    "    data['predict']=pred_y\n",
    " \n",
    "    count=0\n",
    "    \n",
    "    print(\"accuracy_score Score on test data:\")\n",
    "    print(accuracy_score(test_labels, pred_y))\n",
    "    \n",
    "    print(\"f1_score  on test data:\")\n",
    "    print(f1_score(test_labels, pred_y, average='macro'))\n",
    "    filename='result_'+str(clf)[0:15]+'.xlsx'\n",
    "    data.to_excel(filename)\n",
    "    \n",
    "    \n",
    "    sentiment_fit=clf.fit(train_features,train_labels)\n",
    "    y_pred=sentiment_fit.predict(test_features)\n",
    "    print(classification_report(test_labels,y_pred,target_names=('posative','negative')))\n",
    "    \n",
    "    \n",
    "\n",
    "#LOGASTIC REGRESSION\n",
    "\n",
    "\n",
    "\n",
    "logistic_reg = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(logistic_reg, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "\n",
    "# MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "train_n_test_classifier(mnb,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "# S V M\n",
    "\n",
    "svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(svm,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "# MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(mlp,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "# KNN\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "train_n_test_classifier(knn, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "#TREE\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "train_n_test_classifier(dtree, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "estimator = []\n",
    "\n",
    "\n",
    "estimator.append(('LR', LogisticRegression(solver ='lbfgs',  multi_class ='multinomial',  max_iter = 200)))\n",
    "estimator.append(('SVC', SVC(gamma ='auto', probability = True)))\n",
    "estimator.append(('DTC', DecisionTreeClassifier()))\n",
    "estimator.append(('mnb',MultinomialNB()))\n",
    "estimator.append(('mlp',MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)))\n",
    "estimator.append(('knn',KNeighborsClassifier(n_neighbors=3)))     \n",
    "\n",
    "\n",
    "vot_hard = VotingClassifier(estimators = estimator, voting ='hard')\n",
    "\n",
    "train_n_test_classifier(vot_hard, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "vot_soft = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "\n",
    "train_n_test_classifier(vot_soft, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "Gradientclf = GradientBoostingClassifier(n_estimators=100,learning_rate=1.0,max_depth=1, random_state=0)\n",
    "\n",
    "train_n_test_classifier(Gradientclf,train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "adaClf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "train_n_test_classifier(adaClf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(rf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69418022",
   "metadata": {},
   "source": [
    "# # posative negative on lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebf0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM SPLIT\n",
    "\n",
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "#LABE4L FEATURE\n",
    "\n",
    "\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "  # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, test_data = random_split(data, features, output, train_fraction, rand_seed)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "\n",
    "print(len(train_data)+len(test_data))\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "# TF IDF\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True, max_df=0.5, stop_words=None, use_idf=True)\n",
    "train_data_features = vectorizer.fit_transform(train_data['lemma'])\n",
    "test_data_features = vectorizer.transform(test_data['lemma'])\n",
    "\n",
    "#SHAPE\n",
    "\n",
    "train_data_features.shape, test_data_features.shape\n",
    "\n",
    "#FUNCTION FOR MODEL TRAIN\n",
    "\n",
    "\n",
    "def train_n_test_classifier(clf, train_features, train_labels, test_features, test_labels,data):\n",
    "    clf.fit(train_features, train_labels) # please learn patterns from the data\n",
    "\n",
    "   \n",
    "    print('-'*100+str(clf)[0:15])\n",
    "    print(\"accuracy_score Score on training data:\")\n",
    "    print(clf.score(train_features, train_labels))\n",
    "    \n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "    print(\"score on testing data:\")\n",
    "    \n",
    "    pred_y = clf.predict(test_features)\n",
    "    \n",
    "    data['predict']=pred_y\n",
    " \n",
    "    count=0\n",
    "    \n",
    "    print(\"accuracy_score Score on test data:\")\n",
    "    print(accuracy_score(test_labels, pred_y))\n",
    "    \n",
    "    print(\"f1_score  on test data:\")\n",
    "    print(f1_score(test_labels, pred_y, average='macro'))\n",
    "    filename='result_'+str(clf)[0:15]+'.xlsx'\n",
    "    data.to_excel(filename)\n",
    "    \n",
    "    \n",
    "    sentiment_fit=clf.fit(train_features,train_labels)\n",
    "    y_pred=sentiment_fit.predict(test_features)\n",
    "    print(classification_report(test_labels,y_pred,target_names=('posative','negative')))\n",
    "    \n",
    "    \n",
    "\n",
    "#LOGASTIC REGRESSION\n",
    "\n",
    "\n",
    "\n",
    "logistic_reg = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(logistic_reg, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "\n",
    "# MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "train_n_test_classifier(mnb,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "# S V M\n",
    "\n",
    "svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(svm,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "# MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(mlp,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "# KNN\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "train_n_test_classifier(knn, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "#TREE\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "train_n_test_classifier(dtree, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "estimator = []\n",
    "\n",
    "\n",
    "estimator.append(('LR', LogisticRegression(solver ='lbfgs',  multi_class ='multinomial',  max_iter = 200)))\n",
    "estimator.append(('SVC', SVC(gamma ='auto', probability = True)))\n",
    "estimator.append(('DTC', DecisionTreeClassifier()))\n",
    "estimator.append(('mnb',MultinomialNB()))\n",
    "estimator.append(('mlp',MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)))\n",
    "estimator.append(('knn',KNeighborsClassifier(n_neighbors=3)))     \n",
    "\n",
    "\n",
    "vot_hard = VotingClassifier(estimators = estimator, voting ='hard')\n",
    "\n",
    "train_n_test_classifier(vot_hard, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "vot_soft = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "\n",
    "train_n_test_classifier(vot_soft, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "Gradientclf = GradientBoostingClassifier(n_estimators=100,learning_rate=1.0,max_depth=1, random_state=0)\n",
    "\n",
    "train_n_test_classifier(Gradientclf,train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "adaClf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "train_n_test_classifier(adaClf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(rf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bde721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM SPLIT\n",
    "\n",
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "#LABE4L FEATURE\n",
    "\n",
    "\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "  # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, test_data = random_split(data, features, output, train_fraction, rand_seed)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "\n",
    "print(len(train_data)+len(test_data))\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "# BOW\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "train_data_features = vectorizer.fit_transform(train_data['lemma'])\n",
    "\n",
    "test_data_features = vectorizer.transform(test_data['lemma'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#SHAPE\n",
    "\n",
    "train_data_features.shape, test_data_features.shape\n",
    "\n",
    "#FUNCTION FOR MODEL TRAIN\n",
    "\n",
    "\n",
    "def train_n_test_classifier(clf, train_features, train_labels, test_features, test_labels,data):\n",
    "    clf.fit(train_features, train_labels) # please learn patterns from the data\n",
    "\n",
    "   \n",
    "    print('-'*100+str(clf)[0:15])\n",
    "    print(\"accuracy_score Score on training data:\")\n",
    "    print(clf.score(train_features, train_labels))\n",
    "    \n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "    print(\"score on testing data:\")\n",
    "    \n",
    "    pred_y = clf.predict(test_features)\n",
    "    \n",
    "    data['predict']=pred_y\n",
    " \n",
    "    count=0\n",
    "    \n",
    "    print(\"accuracy_score Score on test data:\")\n",
    "    print(accuracy_score(test_labels, pred_y))\n",
    "    \n",
    "    print(\"f1_score  on test data:\")\n",
    "    print(f1_score(test_labels, pred_y, average='macro'))\n",
    "    filename='result_'+str(clf)[0:15]+'.xlsx'\n",
    "    data.to_excel(filename)\n",
    "    \n",
    "    \n",
    "    sentiment_fit=clf.fit(train_features,train_labels)\n",
    "    y_pred=sentiment_fit.predict(test_features)\n",
    "    print(classification_report(test_labels,y_pred,target_names=('posative','negative')))\n",
    "    \n",
    "    \n",
    "\n",
    "#LOGASTIC REGRESSION\n",
    "\n",
    "\n",
    "\n",
    "logistic_reg = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(logistic_reg, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "\n",
    "# MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "train_n_test_classifier(mnb,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "# S V M\n",
    "\n",
    "svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(svm,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "# MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(mlp,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "# KNN\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "train_n_test_classifier(knn, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "#TREE\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "train_n_test_classifier(dtree, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "estimator = []\n",
    "\n",
    "\n",
    "estimator.append(('LR', LogisticRegression(solver ='lbfgs',  multi_class ='multinomial',  max_iter = 200)))\n",
    "estimator.append(('SVC', SVC(gamma ='auto', probability = True)))\n",
    "estimator.append(('DTC', DecisionTreeClassifier()))\n",
    "estimator.append(('mnb',MultinomialNB()))\n",
    "estimator.append(('mlp',MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)))\n",
    "estimator.append(('knn',KNeighborsClassifier(n_neighbors=3)))     \n",
    "\n",
    "\n",
    "vot_hard = VotingClassifier(estimators = estimator, voting ='hard')\n",
    "\n",
    "train_n_test_classifier(vot_hard, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "vot_soft = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "\n",
    "train_n_test_classifier(vot_soft, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "Gradientclf = GradientBoostingClassifier(n_estimators=100,learning_rate=1.0,max_depth=1, random_state=0)\n",
    "\n",
    "train_n_test_classifier(Gradientclf,train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "adaClf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "train_n_test_classifier(adaClf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(rf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a94126",
   "metadata": {},
   "source": [
    "# posative negative on clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba98b3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM SPLIT\n",
    "\n",
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "#LABE4L FEATURE\n",
    "\n",
    "\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "  # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, test_data = random_split(data, features, output, train_fraction, rand_seed)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "\n",
    "print(len(train_data)+len(test_data))\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "\n",
    "# TFIDF\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True, max_df=0.5, stop_words=None, use_idf=True)\n",
    "train_data_features = vectorizer.fit_transform(train_data['clean'])\n",
    "test_data_features = vectorizer.transform(test_data['clean'])\n",
    "\n",
    "#SHAPE\n",
    "\n",
    "train_data_features.shape,  test_data_features.shape\n",
    "\n",
    "#FUNCTION FOR MODEL TRAIN\n",
    "\n",
    "\n",
    "def train_n_test_classifier(clf, train_features, train_labels, test_features, test_labels,data):\n",
    "    clf.fit(train_features, train_labels) # please learn patterns from the data\n",
    "\n",
    "   \n",
    "    print('-'*100+str(clf)[0:15])\n",
    "    print(\"accuracy_score Score on training data:\")\n",
    "    print(clf.score(train_features, train_labels))\n",
    "    \n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "    print(\"score on testing data:\")\n",
    "    \n",
    "    pred_y = clf.predict(test_features)\n",
    "    \n",
    "    data['predict']=pred_y\n",
    " \n",
    "    count=0\n",
    "    \n",
    "    print(\"accuracy_score Score on test data:\")\n",
    "    print(accuracy_score(test_labels, pred_y))\n",
    "    \n",
    "    print(\"f1_score  on test data:\")\n",
    "    print(f1_score(test_labels, pred_y, average='macro'))\n",
    "    filename='result_'+str(clf)[0:15]+'.xlsx'\n",
    "    data.to_excel(filename)\n",
    "    \n",
    "    \n",
    "    sentiment_fit=clf.fit(train_features,train_labels)\n",
    "    y_pred=sentiment_fit.predict(test_features)\n",
    "    print(classification_report(test_labels,y_pred,target_names=('posative','negative')))\n",
    "    \n",
    "    \n",
    "\n",
    "#LOGASTIC REGRESSION\n",
    "\n",
    "\n",
    "\n",
    "logistic_reg = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(logistic_reg, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "\n",
    "# MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "train_n_test_classifier(mnb,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "# S V M\n",
    "\n",
    "svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(svm,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "# MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(mlp,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "# KNN\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "train_n_test_classifier(knn, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "#TREE\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "train_n_test_classifier(dtree, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "estimator = []\n",
    "\n",
    "\n",
    "estimator.append(('LR', LogisticRegression(solver ='lbfgs',  multi_class ='multinomial',  max_iter = 200)))\n",
    "estimator.append(('SVC', SVC(gamma ='auto', probability = True)))\n",
    "estimator.append(('DTC', DecisionTreeClassifier()))\n",
    "estimator.append(('mnb',MultinomialNB()))\n",
    "estimator.append(('mlp',MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)))\n",
    "estimator.append(('knn',KNeighborsClassifier(n_neighbors=3)))     \n",
    "\n",
    "\n",
    "vot_hard = VotingClassifier(estimators = estimator, voting ='hard')\n",
    "\n",
    "train_n_test_classifier(vot_hard, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "vot_soft = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "\n",
    "train_n_test_classifier(vot_soft, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "Gradientclf = GradientBoostingClassifier(n_estimators=100,learning_rate=1.0,max_depth=1, random_state=0)\n",
    "\n",
    "train_n_test_classifier(Gradientclf,train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "adaClf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "train_n_test_classifier(adaClf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(rf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5432fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM SPLIT\n",
    "\n",
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "#LABE4L FEATURE\n",
    "\n",
    "\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "  # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, test_data = random_split(data, features, output, train_fraction, rand_seed)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "\n",
    "print(len(train_data)+len(test_data))\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "\n",
    "# BOW\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "train_data_features = vectorizer.fit_transform(train_data['clean'])\n",
    "\n",
    "test_data_features = vectorizer.transform(test_data['clean'])\n",
    "\n",
    "#SHAPE\n",
    "\n",
    "train_data_features.shape,  test_data_features.shape\n",
    "\n",
    "#FUNCTION FOR MODEL TRAIN\n",
    "\n",
    "\n",
    "def train_n_test_classifier(clf, train_features, train_labels, test_features, test_labels,data):\n",
    "    clf.fit(train_features, train_labels) # please learn patterns from the data\n",
    "\n",
    "   \n",
    "    print('-'*100+str(clf)[0:15])\n",
    "    print(\"accuracy_score Score on training data:\")\n",
    "    print(clf.score(train_features, train_labels))\n",
    "    \n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "    print(\"score on testing data:\")\n",
    "    \n",
    "    pred_y = clf.predict(test_features)\n",
    "    \n",
    "    data['predict']=pred_y\n",
    " \n",
    "    count=0\n",
    "    \n",
    "    print(\"accuracy_score Score on test data:\")\n",
    "    print(accuracy_score(test_labels, pred_y))\n",
    "    \n",
    "    print(\"f1_score  on test data:\")\n",
    "    print(f1_score(test_labels, pred_y, average='macro'))\n",
    "    filename='result_'+str(clf)[0:15]+'.xlsx'\n",
    "    data.to_excel(filename)\n",
    "    \n",
    "    \n",
    "    sentiment_fit=clf.fit(train_features,train_labels)\n",
    "    y_pred=sentiment_fit.predict(test_features)\n",
    "    print(classification_report(test_labels,y_pred,target_names=('posative','negative')))\n",
    "    \n",
    "    \n",
    "\n",
    "#LOGASTIC REGRESSION\n",
    "\n",
    "\n",
    "\n",
    "logistic_reg = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(logistic_reg, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "\n",
    "# MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "train_n_test_classifier(mnb,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "# S V M\n",
    "\n",
    "svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(svm,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "# MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(mlp,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "# KNN\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "train_n_test_classifier(knn, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "#TREE\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "train_n_test_classifier(dtree, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "estimator = []\n",
    "\n",
    "\n",
    "estimator.append(('LR', LogisticRegression(solver ='lbfgs',  multi_class ='multinomial',  max_iter = 200)))\n",
    "estimator.append(('SVC', SVC(gamma ='auto', probability = True)))\n",
    "estimator.append(('DTC', DecisionTreeClassifier()))\n",
    "estimator.append(('mnb',MultinomialNB()))\n",
    "estimator.append(('mlp',MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)))\n",
    "estimator.append(('knn',KNeighborsClassifier(n_neighbors=3)))     \n",
    "\n",
    "\n",
    "vot_hard = VotingClassifier(estimators = estimator, voting ='hard')\n",
    "\n",
    "train_n_test_classifier(vot_hard, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "vot_soft = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "\n",
    "train_n_test_classifier(vot_soft, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "Gradientclf = GradientBoostingClassifier(n_estimators=100,learning_rate=1.0,max_depth=1, random_state=0)\n",
    "\n",
    "train_n_test_classifier(Gradientclf,train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "adaClf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "train_n_test_classifier(adaClf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(rf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf0c54",
   "metadata": {},
   "source": [
    "# tow classfier(neutral, non-neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading our prepared data\n",
    "data = pd.read_excel('data/datasix.xlsx')\n",
    "data = data.dropna()\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc20a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = data[data['label'] == 'ايجابي'].dropna()\n",
    "negative_data = data[data['label'] == 'سلبي'].dropna()\n",
    "neutral_data = data[data['label'] == 'محايد'].dropna()\n",
    "len(positive_data), len(negative_data), len(neutral_data)\n",
    "\n",
    "non_neutral_data = negative_data.append(positive_data).sample(frac=1).reset_index(drop=True)\n",
    "non_neutral_data['label'] = 'غير محايد'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdf4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = non_neutral_data.append(neutral_data).dropna().sample(frac=1).reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b84b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading our prepared data\n",
    "\n",
    "print(data['label'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7589c9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cde2219",
   "metadata": {},
   "source": [
    "# neutral non-neutral  on comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10186b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM SPLIT\n",
    "\n",
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "#LABE4L FEATURE\n",
    "\n",
    "\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "  # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, test_data = random_split(data, features, output, train_fraction, rand_seed)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "\n",
    "print(len(train_data)+len(test_data))\n",
    "print(len(data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b4151",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testNE=test_data[test_data['label'] == 'محايد']\n",
    "testNg=test_data[test_data['label'] == 'غير محايد']\n",
    "\n",
    "print(len(test_data))\n",
    "print(len(testNE),len(testNg))\n",
    "print(\"_______________\")\n",
    "\n",
    "\n",
    "\n",
    "trainNE=train_data[train_data['label'] == 'محايد']\n",
    "trainNg=train_data[train_data['label'] == 'غير محايد']\n",
    "print(len(train_data))\n",
    "print(len(trainNE),len(trainNg))\n",
    "\n",
    "print(\"__________________\")\n",
    "\n",
    "trainNE=data[data['label'] == 'محايد']\n",
    "trainNg=data[data['label'] == 'غير محايد']\n",
    "print(len(data))\n",
    "print(len(trainNE),len(trainNg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a9e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM SPLIT\n",
    "\n",
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "#LABE4L FEATURE\n",
    "\n",
    "\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "  # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, test_data = random_split(data, features, output, train_fraction, rand_seed)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "\n",
    "print(len(train_data)+len(test_data))\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "\n",
    "# TFIDF\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True, max_df=0.5, stop_words=None, use_idf=True)\n",
    "train_data_features = vectorizer.fit_transform(train_data['comment'])\n",
    "test_data_features = vectorizer.transform(test_data['comment'])\n",
    "\n",
    "#SHAPE\n",
    "\n",
    "train_data_features.shape,  test_data_features.shape\n",
    "\n",
    "#FUNCTION FOR MODEL TRAIN\n",
    "\n",
    "\n",
    "def train_n_test_classifier(clf, train_features, train_labels, test_features, test_labels,data):\n",
    "    clf.fit(train_features, train_labels) # please learn patterns from the data\n",
    "\n",
    "   \n",
    "    print('-'*100+str(clf)[0:15])\n",
    "    print(\"accuracy_score Score on training data:\")\n",
    "    print(clf.score(train_features, train_labels))\n",
    "    \n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "    print(\"score on testing data:\")\n",
    "    \n",
    "    pred_y = clf.predict(test_features)\n",
    "    \n",
    "    data['predict']=pred_y\n",
    " \n",
    "    count=0\n",
    "    \n",
    "    print(\"accuracy_score Score on test data:\")\n",
    "    print(accuracy_score(test_labels, pred_y))\n",
    "    \n",
    "    print(\"f1_score  on test data:\")\n",
    "    print(f1_score(test_labels, pred_y, average='macro'))\n",
    "    filename='result_'+str(clf)[0:15]+'.xlsx'\n",
    "    data.to_excel(filename)\n",
    "    \n",
    "    \n",
    "    sentiment_fit=clf.fit(train_features,train_labels)\n",
    "    y_pred=sentiment_fit.predict(test_features)\n",
    "    print(classification_report(test_labels,y_pred,target_names=('neutral','non-neutral')))\n",
    "    \n",
    "    \n",
    "\n",
    "#LOGASTIC REGRESSION\n",
    "\n",
    "\n",
    "\n",
    "logistic_reg = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(logistic_reg, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "\n",
    "# MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "train_n_test_classifier(mnb,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "# S V M\n",
    "\n",
    "svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(svm,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "# MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(mlp,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "# KNN\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "train_n_test_classifier(knn, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "#TREE\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "train_n_test_classifier(dtree, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "estimator = []\n",
    "\n",
    "\n",
    "estimator.append(('LR', LogisticRegression(solver ='lbfgs',  multi_class ='multinomial',  max_iter = 200)))\n",
    "estimator.append(('SVC', SVC(gamma ='auto', probability = True)))\n",
    "estimator.append(('DTC', DecisionTreeClassifier()))\n",
    "estimator.append(('mnb',MultinomialNB()))\n",
    "estimator.append(('mlp',MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)))\n",
    "estimator.append(('knn',KNeighborsClassifier(n_neighbors=3)))     \n",
    "\n",
    "\n",
    "vot_hard = VotingClassifier(estimators = estimator, voting ='hard')\n",
    "\n",
    "train_n_test_classifier(vot_hard, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "vot_soft = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "\n",
    "train_n_test_classifier(vot_soft, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "Gradientclf = GradientBoostingClassifier(n_estimators=100,learning_rate=1.0,max_depth=1, random_state=0)\n",
    "\n",
    "train_n_test_classifier(Gradientclf,train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "adaClf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "train_n_test_classifier(adaClf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(rf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa87b18",
   "metadata": {},
   "source": [
    "#  neutral non-neutral  on comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8a2d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM SPLIT\n",
    "\n",
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "#LABE4L FEATURE\n",
    "\n",
    "\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "  # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, test_data = random_split(data, features, output, train_fraction, rand_seed)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "\n",
    "print(len(train_data)+len(test_data))\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "\n",
    "# BOW\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "train_data_features = vectorizer.fit_transform(train_data['comment'])\n",
    "test_data_features = vectorizer.transform(test_data['comment'])\n",
    "\n",
    "#SHAPE\n",
    "\n",
    "train_data_features.shape,  test_data_features.shape\n",
    "\n",
    "#FUNCTION FOR MODEL TRAIN\n",
    "\n",
    "\n",
    "def train_n_test_classifier(clf, train_features, train_labels, test_features, test_labels,data):\n",
    "    clf.fit(train_features, train_labels) # please learn patterns from the data\n",
    "\n",
    "   \n",
    "    print('-'*100+str(clf)[0:15])\n",
    "    print(\"accuracy_score Score on training data:\")\n",
    "    print(clf.score(train_features, train_labels))\n",
    "    \n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "    print(\"score on testing data:\")\n",
    "    \n",
    "    pred_y = clf.predict(test_features)\n",
    "    \n",
    "    data['predict']=pred_y\n",
    " \n",
    "    count=0\n",
    "    \n",
    "    print(\"accuracy_score Score on test data:\")\n",
    "    print(accuracy_score(test_labels, pred_y))\n",
    "    \n",
    "    print(\"f1_score  on test data:\")\n",
    "    print(f1_score(test_labels, pred_y, average='macro'))\n",
    "    filename='result_'+str(clf)[0:15]+'.xlsx'\n",
    "    data.to_excel(filename)\n",
    "    \n",
    "    \n",
    "    sentiment_fit=clf.fit(train_features,train_labels)\n",
    "    y_pred=sentiment_fit.predict(test_features)\n",
    "    print(classification_report(test_labels,y_pred,target_names=('neutral','non-neutral')))\n",
    "    \n",
    "    \n",
    "\n",
    "#LOGASTIC REGRESSION\n",
    "\n",
    "\n",
    "\n",
    "logistic_reg = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(logistic_reg, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "\n",
    "# MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "train_n_test_classifier(mnb,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "# S V M\n",
    "\n",
    "svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(svm,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "# MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(mlp,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "# KNN\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "train_n_test_classifier(knn, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "#TREE\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "train_n_test_classifier(dtree, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "estimator = []\n",
    "\n",
    "\n",
    "estimator.append(('LR', LogisticRegression(solver ='lbfgs',  multi_class ='multinomial',  max_iter = 200)))\n",
    "estimator.append(('SVC', SVC(gamma ='auto', probability = True)))\n",
    "estimator.append(('DTC', DecisionTreeClassifier()))\n",
    "estimator.append(('mnb',MultinomialNB()))\n",
    "estimator.append(('mlp',MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)))\n",
    "estimator.append(('knn',KNeighborsClassifier(n_neighbors=3)))     \n",
    "\n",
    "\n",
    "vot_hard = VotingClassifier(estimators = estimator, voting ='hard')\n",
    "\n",
    "train_n_test_classifier(vot_hard, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "vot_soft = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "\n",
    "train_n_test_classifier(vot_soft, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "Gradientclf = GradientBoostingClassifier(n_estimators=100,learning_rate=1.0,max_depth=1, random_state=0)\n",
    "\n",
    "train_n_test_classifier(Gradientclf,train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "adaClf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "train_n_test_classifier(adaClf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(rf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df37b03f",
   "metadata": {},
   "source": [
    "#  neutral non-neutral  on comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f71c92d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: label\n",
      "features: ['comment', 'lemma', 'clean']\n",
      "4928\n",
      "1233\n",
      "6161\n",
      "6161\n",
      "----------------------------------------------------------------------------------------------------LogisticRegress\n",
      "accuracy_score Score on training data:\n",
      "0.8409090909090909\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8085969180859692\n",
      "f1_score  on test data:\n",
      "0.5994152046783627\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.81      0.98      0.89       961\n",
      " non-neutral       0.76      0.19      0.31       272\n",
      "\n",
      "    accuracy                           0.81      1233\n",
      "   macro avg       0.78      0.59      0.60      1233\n",
      "weighted avg       0.80      0.81      0.76      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------MultinomialNB()\n",
      "accuracy_score Score on training data:\n",
      "0.8352272727272727\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7988645579886455\n",
      "f1_score  on test data:\n",
      "0.5321026796097489\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.80      1.00      0.89       961\n",
      " non-neutral       0.90      0.10      0.18       272\n",
      "\n",
      "    accuracy                           0.80      1233\n",
      "   macro avg       0.85      0.55      0.53      1233\n",
      "weighted avg       0.82      0.80      0.73      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------SVC(kernel='lin\n",
      "accuracy_score Score on training data:\n",
      "0.9648944805194806\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8337388483373885\n",
      "f1_score  on test data:\n",
      "0.7064098171195605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.85      0.96      0.90       961\n",
      " non-neutral       0.72      0.40      0.51       272\n",
      "\n",
      "    accuracy                           0.83      1233\n",
      "   macro avg       0.79      0.68      0.71      1233\n",
      "weighted avg       0.82      0.83      0.81      1233\n",
      "\n",
      "Iteration 1, loss = 0.80112399\n",
      "Iteration 2, loss = 0.69365701\n",
      "Iteration 3, loss = 0.61347923\n",
      "Iteration 4, loss = 0.52845836\n",
      "Iteration 5, loss = 0.44147898\n",
      "Iteration 6, loss = 0.32285280\n",
      "Iteration 7, loss = 0.18336833\n",
      "Iteration 8, loss = 0.09630520\n",
      "Iteration 9, loss = 0.05525567\n",
      "Iteration 10, loss = 0.03826311\n",
      "Iteration 11, loss = 0.03035316\n",
      "Iteration 12, loss = 0.02656823\n",
      "Iteration 13, loss = 0.02418489\n",
      "Iteration 14, loss = 0.02326739\n",
      "Iteration 15, loss = 0.02305089\n",
      "Iteration 16, loss = 0.02159776\n",
      "Iteration 17, loss = 0.02106558\n",
      "Iteration 18, loss = 0.02072290\n",
      "Iteration 19, loss = 0.02137982\n",
      "Iteration 20, loss = 0.02097216\n",
      "Iteration 21, loss = 0.02230143\n",
      "Iteration 22, loss = 0.02007001\n",
      "Iteration 23, loss = 0.02030985\n",
      "Iteration 24, loss = 0.02010952\n",
      "Iteration 25, loss = 0.02183377\n",
      "Iteration 26, loss = 0.02098370\n",
      "Iteration 27, loss = 0.02059519\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "----------------------------------------------------------------------------------------------------MLPClassifier(h\n",
      "accuracy_score Score on training data:\n",
      "0.9900568181818182\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8369829683698297\n",
      "f1_score  on test data:\n",
      "0.7428512730694883\n",
      "Iteration 1, loss = 0.80112399\n",
      "Iteration 2, loss = 0.69365701\n",
      "Iteration 3, loss = 0.61347923\n",
      "Iteration 4, loss = 0.52845836\n",
      "Iteration 5, loss = 0.44147898\n",
      "Iteration 6, loss = 0.32285280\n",
      "Iteration 7, loss = 0.18336833\n",
      "Iteration 8, loss = 0.09630520\n",
      "Iteration 9, loss = 0.05525567\n",
      "Iteration 10, loss = 0.03826311\n",
      "Iteration 11, loss = 0.03035316\n",
      "Iteration 12, loss = 0.02656823\n",
      "Iteration 13, loss = 0.02418489\n",
      "Iteration 14, loss = 0.02326739\n",
      "Iteration 15, loss = 0.02305089\n",
      "Iteration 16, loss = 0.02159776\n",
      "Iteration 17, loss = 0.02106558\n",
      "Iteration 18, loss = 0.02072290\n",
      "Iteration 19, loss = 0.02137982\n",
      "Iteration 20, loss = 0.02097216\n",
      "Iteration 21, loss = 0.02230143\n",
      "Iteration 22, loss = 0.02007001\n",
      "Iteration 23, loss = 0.02030985\n",
      "Iteration 24, loss = 0.02010952\n",
      "Iteration 25, loss = 0.02183377\n",
      "Iteration 26, loss = 0.02098370\n",
      "Iteration 27, loss = 0.02059519\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.87      0.93      0.90       961\n",
      " non-neutral       0.67      0.53      0.59       272\n",
      "\n",
      "    accuracy                           0.84      1233\n",
      "   macro avg       0.77      0.73      0.74      1233\n",
      "weighted avg       0.83      0.84      0.83      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------KNeighborsClass\n",
      "accuracy_score Score on training data:\n",
      "0.8993506493506493\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8085969180859692\n",
      "f1_score  on test data:\n",
      "0.6724082029666856\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.84      0.93      0.88       961\n",
      " non-neutral       0.61      0.37      0.46       272\n",
      "\n",
      "    accuracy                           0.81      1233\n",
      "   macro avg       0.72      0.65      0.67      1233\n",
      "weighted avg       0.79      0.81      0.79      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------DecisionTreeCla\n",
      "accuracy_score Score on training data:\n",
      "0.9900568181818182\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7761557177615572\n",
      "f1_score  on test data:\n",
      "0.6665085649327741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.85      0.87      0.86       961\n",
      " non-neutral       0.49      0.46      0.48       272\n",
      "\n",
      "    accuracy                           0.78      1233\n",
      "   macro avg       0.67      0.66      0.67      1233\n",
      "weighted avg       0.77      0.78      0.77      1233\n",
      "\n",
      "Iteration 1, loss = 0.80112399\n",
      "Iteration 2, loss = 0.69365701\n",
      "Iteration 3, loss = 0.61347923\n",
      "Iteration 4, loss = 0.52845836\n",
      "Iteration 5, loss = 0.44147898\n",
      "Iteration 6, loss = 0.32285280\n",
      "Iteration 7, loss = 0.18336833\n",
      "Iteration 8, loss = 0.09630520\n",
      "Iteration 9, loss = 0.05525567\n",
      "Iteration 10, loss = 0.03826311\n",
      "Iteration 11, loss = 0.03035316\n",
      "Iteration 12, loss = 0.02656823\n",
      "Iteration 13, loss = 0.02418489\n",
      "Iteration 14, loss = 0.02326739\n",
      "Iteration 15, loss = 0.02305089\n",
      "Iteration 16, loss = 0.02159776\n",
      "Iteration 17, loss = 0.02106558\n",
      "Iteration 18, loss = 0.02072290\n",
      "Iteration 19, loss = 0.02137982\n",
      "Iteration 20, loss = 0.02097216\n",
      "Iteration 21, loss = 0.02230143\n",
      "Iteration 22, loss = 0.02007001\n",
      "Iteration 23, loss = 0.02030985\n",
      "Iteration 24, loss = 0.02010952\n",
      "Iteration 25, loss = 0.02183377\n",
      "Iteration 26, loss = 0.02098370\n",
      "Iteration 27, loss = 0.02059519\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "----------------------------------------------------------------------------------------------------VotingClassifie\n",
      "accuracy_score Score on training data:\n",
      "0.8839285714285714\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.805352798053528\n",
      "f1_score  on test data:\n",
      "0.5865191146881288\n",
      "Iteration 1, loss = 0.80112399\n",
      "Iteration 2, loss = 0.69365701\n",
      "Iteration 3, loss = 0.61347923\n",
      "Iteration 4, loss = 0.52845836\n",
      "Iteration 5, loss = 0.44147898\n",
      "Iteration 6, loss = 0.32285280\n",
      "Iteration 7, loss = 0.18336833\n",
      "Iteration 8, loss = 0.09630520\n",
      "Iteration 9, loss = 0.05525567\n",
      "Iteration 10, loss = 0.03826311\n",
      "Iteration 11, loss = 0.03035316\n",
      "Iteration 12, loss = 0.02656823\n",
      "Iteration 13, loss = 0.02418489\n",
      "Iteration 14, loss = 0.02326739\n",
      "Iteration 15, loss = 0.02305089\n",
      "Iteration 16, loss = 0.02159776\n",
      "Iteration 18, loss = 0.02072290\n",
      "Iteration 19, loss = 0.02137982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.02097216\n",
      "Iteration 21, loss = 0.02230143\n",
      "Iteration 22, loss = 0.02007001\n",
      "Iteration 23, loss = 0.02030985\n",
      "Iteration 24, loss = 0.02010952\n",
      "Iteration 25, loss = 0.02183377\n",
      "Iteration 26, loss = 0.02098370\n",
      "Iteration 27, loss = 0.02059519\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.81      0.98      0.89       961\n",
      " non-neutral       0.73      0.17      0.28       272\n",
      "\n",
      "    accuracy                           0.80      1233\n",
      "   macro avg       0.77      0.58      0.58      1233\n",
      "weighted avg       0.79      0.80      0.75      1233\n",
      "\n",
      "Iteration 1, loss = 0.80112399\n",
      "Iteration 2, loss = 0.69365701\n",
      "Iteration 3, loss = 0.61347923\n",
      "Iteration 4, loss = 0.52845836\n",
      "Iteration 5, loss = 0.44147898\n",
      "Iteration 6, loss = 0.32285280\n",
      "Iteration 7, loss = 0.18336833\n",
      "Iteration 8, loss = 0.09630520\n",
      "Iteration 9, loss = 0.05525567\n",
      "Iteration 10, loss = 0.03826311\n",
      "Iteration 11, loss = 0.03035316\n",
      "Iteration 12, loss = 0.02656823\n",
      "Iteration 13, loss = 0.02418489\n",
      "Iteration 14, loss = 0.02326739\n",
      "Iteration 15, loss = 0.02305089\n",
      "Iteration 16, loss = 0.02159776\n",
      "Iteration 17, loss = 0.02106558\n",
      "Iteration 18, loss = 0.02072290\n",
      "Iteration 19, loss = 0.02137982\n",
      "Iteration 20, loss = 0.02097216\n",
      "Iteration 21, loss = 0.02230143\n",
      "Iteration 22, loss = 0.02007001\n",
      "Iteration 23, loss = 0.02030985\n",
      "Iteration 24, loss = 0.02010952\n",
      "Iteration 25, loss = 0.02183377\n",
      "Iteration 26, loss = 0.02098370\n",
      "Iteration 27, loss = 0.02059519\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "----------------------------------------------------------------------------------------------------VotingClassifie\n",
      "accuracy_score Score on training data:\n",
      "0.9717938311688312\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8280616382806164\n",
      "f1_score  on test data:\n",
      "0.6730457492795389\n",
      "Iteration 1, loss = 0.80112399\n",
      "Iteration 2, loss = 0.69365701\n",
      "Iteration 3, loss = 0.61347923\n",
      "Iteration 4, loss = 0.52845836\n",
      "Iteration 5, loss = 0.44147898\n",
      "Iteration 6, loss = 0.32285280\n",
      "Iteration 7, loss = 0.18336833\n",
      "Iteration 8, loss = 0.09630520\n",
      "Iteration 9, loss = 0.05525567\n",
      "Iteration 10, loss = 0.03826311\n",
      "Iteration 11, loss = 0.03035316\n",
      "Iteration 12, loss = 0.02656823\n",
      "Iteration 13, loss = 0.02418489\n",
      "Iteration 14, loss = 0.02326739\n",
      "Iteration 15, loss = 0.02305089\n",
      "Iteration 16, loss = 0.02159776\n",
      "Iteration 17, loss = 0.02106558\n",
      "Iteration 18, loss = 0.02072290\n",
      "Iteration 19, loss = 0.02137982\n",
      "Iteration 20, loss = 0.02097216\n",
      "Iteration 21, loss = 0.02230143\n",
      "Iteration 22, loss = 0.02007001\n",
      "Iteration 23, loss = 0.02030985\n",
      "Iteration 24, loss = 0.02010952\n",
      "Iteration 25, loss = 0.02183377\n",
      "Iteration 26, loss = 0.02098370\n",
      "Iteration 27, loss = 0.02059519\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.83      0.97      0.90       961\n",
      " non-neutral       0.77      0.31      0.44       272\n",
      "\n",
      "    accuracy                           0.83      1233\n",
      "   macro avg       0.80      0.64      0.67      1233\n",
      "weighted avg       0.82      0.83      0.80      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------GradientBoostin\n",
      "accuracy_score Score on training data:\n",
      "0.861810064935065\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7850770478507705\n",
      "f1_score  on test data:\n",
      "0.6630539585965094\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.84      0.89      0.87       961\n",
      " non-neutral       0.52      0.42      0.46       272\n",
      "\n",
      "    accuracy                           0.79      1233\n",
      "   macro avg       0.68      0.65      0.66      1233\n",
      "weighted avg       0.77      0.79      0.78      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------AdaBoostClassif\n",
      "accuracy_score Score on training data:\n",
      "0.8431412337662337\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7907542579075426\n",
      "f1_score  on test data:\n",
      "0.6540975834652656\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.84      0.91      0.87       961\n",
      " non-neutral       0.54      0.37      0.44       272\n",
      "\n",
      "    accuracy                           0.79      1233\n",
      "   macro avg       0.69      0.64      0.65      1233\n",
      "weighted avg       0.77      0.79      0.78      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------RandomForestCla\n",
      "accuracy_score Score on training data:\n",
      "0.989448051948052\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8248175182481752\n",
      "f1_score  on test data:\n",
      "0.6888924192583732\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.84      0.95      0.89       961\n",
      " non-neutral       0.69      0.37      0.48       272\n",
      "\n",
      "    accuracy                           0.82      1233\n",
      "   macro avg       0.77      0.66      0.69      1233\n",
      "weighted avg       0.81      0.82      0.80      1233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RANDOM SPLIT\n",
    "\n",
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "#LABE4L FEATURE\n",
    "\n",
    "\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "  # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, test_data = random_split(data, features, output, train_fraction, rand_seed)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "\n",
    "print(len(train_data)+len(test_data))\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "\n",
    "# TFIDF\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True, max_df=0.5, stop_words=None, use_idf=True)\n",
    "train_data_features = vectorizer.fit_transform(train_data['lemma'])\n",
    "test_data_features = vectorizer.transform(test_data['lemma'])\n",
    "\n",
    "#SHAPE\n",
    "\n",
    "train_data_features.shape,  test_data_features.shape\n",
    "\n",
    "#FUNCTION FOR MODEL TRAIN\n",
    "\n",
    "\n",
    "def train_n_test_classifier(clf, train_features, train_labels, test_features, test_labels,data):\n",
    "    clf.fit(train_features, train_labels) # please learn patterns from the data\n",
    "\n",
    "   \n",
    "    print('-'*100+str(clf)[0:15])\n",
    "    print(\"accuracy_score Score on training data:\")\n",
    "    print(clf.score(train_features, train_labels))\n",
    "    \n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "    print(\"score on testing data:\")\n",
    "    \n",
    "    pred_y = clf.predict(test_features)\n",
    "    \n",
    "    data['predict']=pred_y\n",
    " \n",
    "    count=0\n",
    "    \n",
    "    print(\"accuracy_score Score on test data:\")\n",
    "    print(accuracy_score(test_labels, pred_y))\n",
    "    \n",
    "    print(\"f1_score  on test data:\")\n",
    "    print(f1_score(test_labels, pred_y, average='macro'))\n",
    "    filename='result_'+str(clf)[0:15]+'.xlsx'\n",
    "    data.to_excel(filename)\n",
    "    \n",
    "    \n",
    "    sentiment_fit=clf.fit(train_features,train_labels)\n",
    "    y_pred=sentiment_fit.predict(test_features)\n",
    "    print(classification_report(test_labels,y_pred,target_names=('neutral','non-neutral')))\n",
    "    \n",
    "    \n",
    "\n",
    "#LOGASTIC REGRESSION\n",
    "\n",
    "\n",
    "\n",
    "logistic_reg = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(logistic_reg, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "\n",
    "# MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "train_n_test_classifier(mnb,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "# S V M\n",
    "\n",
    "svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(svm,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "# MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(mlp,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "# KNN\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "train_n_test_classifier(knn, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "#TREE\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "train_n_test_classifier(dtree, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "estimator = []\n",
    "\n",
    "\n",
    "estimator.append(('LR', LogisticRegression(solver ='lbfgs',  multi_class ='multinomial',  max_iter = 200)))\n",
    "estimator.append(('SVC', SVC(gamma ='auto', probability = True)))\n",
    "estimator.append(('DTC', DecisionTreeClassifier()))\n",
    "estimator.append(('mnb',MultinomialNB()))\n",
    "estimator.append(('mlp',MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)))\n",
    "estimator.append(('knn',KNeighborsClassifier(n_neighbors=3)))     \n",
    "\n",
    "\n",
    "vot_hard = VotingClassifier(estimators = estimator, voting ='hard')\n",
    "\n",
    "train_n_test_classifier(vot_hard, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "vot_soft = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "\n",
    "train_n_test_classifier(vot_soft, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "Gradientclf = GradientBoostingClassifier(n_estimators=100,learning_rate=1.0,max_depth=1, random_state=0)\n",
    "\n",
    "train_n_test_classifier(Gradientclf,train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "adaClf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "train_n_test_classifier(adaClf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(rf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b12fd50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: label\n",
      "features: ['comment', 'lemma', 'clean']\n",
      "4928\n",
      "1233\n",
      "6161\n",
      "6161\n",
      "----------------------------------------------------------------------------------------------------LogisticRegress\n",
      "accuracy_score Score on training data:\n",
      "0.9655032467532467\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8248175182481752\n",
      "f1_score  on test data:\n",
      "0.6888924192583732\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.84      0.95      0.89       961\n",
      " non-neutral       0.69      0.37      0.48       272\n",
      "\n",
      "    accuracy                           0.82      1233\n",
      "   macro avg       0.77      0.66      0.69      1233\n",
      "weighted avg       0.81      0.82      0.80      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------MultinomialNB()\n",
      "accuracy_score Score on training data:\n",
      "0.9500811688311688\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8183292781832928\n",
      "f1_score  on test data:\n",
      "0.6438621827934805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.82      0.97      0.89       961\n",
      " non-neutral       0.74      0.27      0.39       272\n",
      "\n",
      "    accuracy                           0.82      1233\n",
      "   macro avg       0.78      0.62      0.64      1233\n",
      "weighted avg       0.81      0.82      0.78      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------SVC(kernel='lin\n",
      "accuracy_score Score on training data:\n",
      "0.984375\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8183292781832928\n",
      "f1_score  on test data:\n",
      "0.7120864641275613\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.86      0.91      0.89       961\n",
      " non-neutral       0.61      0.48      0.54       272\n",
      "\n",
      "    accuracy                           0.82      1233\n",
      "   macro avg       0.74      0.70      0.71      1233\n",
      "weighted avg       0.81      0.82      0.81      1233\n",
      "\n",
      "Iteration 1, loss = 0.69762774\n",
      "Iteration 2, loss = 0.52316287\n",
      "Iteration 3, loss = 0.40993790\n",
      "Iteration 4, loss = 0.30986094\n",
      "Iteration 5, loss = 0.21321527\n",
      "Iteration 6, loss = 0.12874443\n",
      "Iteration 7, loss = 0.07230644\n",
      "Iteration 8, loss = 0.05017188\n",
      "Iteration 9, loss = 0.03845000\n",
      "Iteration 10, loss = 0.03640107\n",
      "Iteration 11, loss = 0.03042919\n",
      "Iteration 12, loss = 0.02859636\n",
      "Iteration 13, loss = 0.02624596\n",
      "Iteration 14, loss = 0.02605752\n",
      "Iteration 15, loss = 0.02457614\n",
      "Iteration 16, loss = 0.02418140\n",
      "Iteration 17, loss = 0.02307789\n",
      "Iteration 18, loss = 0.02369429\n",
      "Iteration 19, loss = 0.02257307\n",
      "Iteration 20, loss = 0.02239378\n",
      "Iteration 21, loss = 0.02148833\n",
      "Iteration 22, loss = 0.02292281\n",
      "Iteration 23, loss = 0.02322481\n",
      "Iteration 24, loss = 0.02263071\n",
      "Iteration 25, loss = 0.02221918\n",
      "Iteration 26, loss = 0.02171068\n",
      "Iteration 27, loss = 0.02273698\n",
      "Iteration 28, loss = 0.02124820\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "----------------------------------------------------------------------------------------------------MLPClassifier(h\n",
      "accuracy_score Score on training data:\n",
      "0.9884334415584416\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8110300081103001\n",
      "f1_score  on test data:\n",
      "0.7448372642452072\n",
      "Iteration 1, loss = 0.69762774\n",
      "Iteration 2, loss = 0.52316287\n",
      "Iteration 3, loss = 0.40993790\n",
      "Iteration 4, loss = 0.30986094\n",
      "Iteration 5, loss = 0.21321527\n",
      "Iteration 6, loss = 0.12874443\n",
      "Iteration 7, loss = 0.07230644\n",
      "Iteration 8, loss = 0.05017188\n",
      "Iteration 9, loss = 0.03845000\n",
      "Iteration 10, loss = 0.03640107\n",
      "Iteration 11, loss = 0.03042919\n",
      "Iteration 12, loss = 0.02859636\n",
      "Iteration 13, loss = 0.02624596\n",
      "Iteration 14, loss = 0.02605752\n",
      "Iteration 15, loss = 0.02457614\n",
      "Iteration 16, loss = 0.02418140\n",
      "Iteration 17, loss = 0.02307789\n",
      "Iteration 18, loss = 0.02369429\n",
      "Iteration 19, loss = 0.02257307\n",
      "Iteration 20, loss = 0.02239378\n",
      "Iteration 21, loss = 0.02148833\n",
      "Iteration 22, loss = 0.02292281\n",
      "Iteration 23, loss = 0.02322481\n",
      "Iteration 24, loss = 0.02263071\n",
      "Iteration 25, loss = 0.02221918\n",
      "Iteration 26, loss = 0.02171068\n",
      "Iteration 27, loss = 0.02273698\n",
      "Iteration 28, loss = 0.02124820\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.90      0.85      0.87       961\n",
      " non-neutral       0.56      0.68      0.61       272\n",
      "\n",
      "    accuracy                           0.81      1233\n",
      "   macro avg       0.73      0.77      0.74      1233\n",
      "weighted avg       0.83      0.81      0.82      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------KNeighborsClass\n",
      "accuracy_score Score on training data:\n",
      "0.9157873376623377\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7745336577453366\n",
      "f1_score  on test data:\n",
      "0.6470642745941138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.84      0.88      0.86       961\n",
      " non-neutral       0.49      0.39      0.43       272\n",
      "\n",
      "    accuracy                           0.77      1233\n",
      "   macro avg       0.66      0.64      0.65      1233\n",
      "weighted avg       0.76      0.77      0.77      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------DecisionTreeCla\n",
      "accuracy_score Score on training data:\n",
      "0.9888392857142857\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7980535279805353\n",
      "f1_score  on test data:\n",
      "0.683397870530305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.85      0.90      0.87       961\n",
      " non-neutral       0.55      0.44      0.49       272\n",
      "\n",
      "    accuracy                           0.80      1233\n",
      "   macro avg       0.70      0.67      0.68      1233\n",
      "weighted avg       0.79      0.80      0.79      1233\n",
      "\n",
      "Iteration 1, loss = 0.69762774\n",
      "Iteration 2, loss = 0.52316287\n",
      "Iteration 3, loss = 0.40993790\n",
      "Iteration 4, loss = 0.30986094\n",
      "Iteration 5, loss = 0.21321527\n",
      "Iteration 6, loss = 0.12874443\n",
      "Iteration 7, loss = 0.07230644\n",
      "Iteration 8, loss = 0.05017188\n",
      "Iteration 9, loss = 0.03845000\n",
      "Iteration 10, loss = 0.03640107\n",
      "Iteration 11, loss = 0.03042919\n",
      "Iteration 12, loss = 0.02859636\n",
      "Iteration 13, loss = 0.02624596\n",
      "Iteration 14, loss = 0.02605752\n",
      "Iteration 15, loss = 0.02457614\n",
      "Iteration 16, loss = 0.02418140\n",
      "Iteration 17, loss = 0.02307789\n",
      "Iteration 18, loss = 0.02369429\n",
      "Iteration 19, loss = 0.02257307\n",
      "Iteration 20, loss = 0.02239378\n",
      "Iteration 21, loss = 0.02148833\n",
      "Iteration 22, loss = 0.02292281\n",
      "Iteration 23, loss = 0.02322481\n",
      "Iteration 24, loss = 0.02263071\n",
      "Iteration 25, loss = 0.02221918\n",
      "Iteration 26, loss = 0.02171068\n",
      "Iteration 27, loss = 0.02273698\n",
      "Iteration 28, loss = 0.02124820\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "----------------------------------------------------------------------------------------------------VotingClassifie\n",
      "accuracy_score Score on training data:\n",
      "0.9736201298701299\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8215733982157339\n",
      "f1_score  on test data:\n",
      "0.6621412913511359\n",
      "Iteration 1, loss = 0.69762774\n",
      "Iteration 2, loss = 0.52316287\n",
      "Iteration 3, loss = 0.40993790\n",
      "Iteration 4, loss = 0.30986094\n",
      "Iteration 5, loss = 0.21321527\n",
      "Iteration 6, loss = 0.12874443\n",
      "Iteration 7, loss = 0.07230644\n",
      "Iteration 8, loss = 0.05017188\n",
      "Iteration 9, loss = 0.03845000\n",
      "Iteration 10, loss = 0.03640107\n",
      "Iteration 11, loss = 0.03042919\n",
      "Iteration 12, loss = 0.02859636\n",
      "Iteration 13, loss = 0.02624596\n",
      "Iteration 14, loss = 0.02605752\n",
      "Iteration 15, loss = 0.02457614\n",
      "Iteration 16, loss = 0.02418140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.02307789\n",
      "Iteration 18, loss = 0.02369429\n",
      "Iteration 19, loss = 0.02257307\n",
      "Iteration 20, loss = 0.02239378\n",
      "Iteration 21, loss = 0.02148833\n",
      "Iteration 22, loss = 0.02292281\n",
      "Iteration 23, loss = 0.02322481\n",
      "Iteration 24, loss = 0.02263071\n",
      "Iteration 25, loss = 0.02221918\n",
      "Iteration 26, loss = 0.02171068\n",
      "Iteration 27, loss = 0.02273698\n",
      "Iteration 28, loss = 0.02124820\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.83      0.97      0.89       961\n",
      " non-neutral       0.72      0.31      0.44       272\n",
      "\n",
      "    accuracy                           0.82      1233\n",
      "   macro avg       0.78      0.64      0.66      1233\n",
      "weighted avg       0.81      0.82      0.79      1233\n",
      "\n",
      "Iteration 1, loss = 0.69762774\n",
      "Iteration 2, loss = 0.52316287\n",
      "Iteration 3, loss = 0.40993790\n",
      "Iteration 4, loss = 0.30986094\n",
      "Iteration 5, loss = 0.21321527\n",
      "Iteration 6, loss = 0.12874443\n",
      "Iteration 7, loss = 0.07230644\n",
      "Iteration 8, loss = 0.05017188\n",
      "Iteration 9, loss = 0.03845000\n",
      "Iteration 10, loss = 0.03640107\n",
      "Iteration 11, loss = 0.03042919\n",
      "Iteration 12, loss = 0.02859636\n",
      "Iteration 13, loss = 0.02624596\n",
      "Iteration 14, loss = 0.02605752\n",
      "Iteration 15, loss = 0.02457614\n",
      "Iteration 16, loss = 0.02418140\n",
      "Iteration 17, loss = 0.02307789\n",
      "Iteration 18, loss = 0.02369429\n",
      "Iteration 19, loss = 0.02257307\n",
      "Iteration 20, loss = 0.02239378\n",
      "Iteration 21, loss = 0.02148833\n",
      "Iteration 22, loss = 0.02292281\n",
      "Iteration 23, loss = 0.02322481\n",
      "Iteration 24, loss = 0.02263071\n",
      "Iteration 25, loss = 0.02221918\n",
      "Iteration 26, loss = 0.02171068\n",
      "Iteration 27, loss = 0.02273698\n",
      "Iteration 28, loss = 0.02124820\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "----------------------------------------------------------------------------------------------------VotingClassifie\n",
      "accuracy_score Score on training data:\n",
      "0.9859983766233766\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8313057583130575\n",
      "f1_score  on test data:\n",
      "0.7153309949829063\n",
      "Iteration 1, loss = 0.69762774\n",
      "Iteration 2, loss = 0.52316287\n",
      "Iteration 3, loss = 0.40993790\n",
      "Iteration 4, loss = 0.30986094\n",
      "Iteration 5, loss = 0.21321527\n",
      "Iteration 6, loss = 0.12874443\n",
      "Iteration 7, loss = 0.07230644\n",
      "Iteration 8, loss = 0.05017188\n",
      "Iteration 9, loss = 0.03845000\n",
      "Iteration 10, loss = 0.03640107\n",
      "Iteration 11, loss = 0.03042919\n",
      "Iteration 12, loss = 0.02859636\n",
      "Iteration 13, loss = 0.02624596\n",
      "Iteration 14, loss = 0.02605752\n",
      "Iteration 15, loss = 0.02457614\n",
      "Iteration 16, loss = 0.02418140\n",
      "Iteration 17, loss = 0.02307789\n",
      "Iteration 18, loss = 0.02369429\n",
      "Iteration 19, loss = 0.02257307\n",
      "Iteration 20, loss = 0.02239378\n",
      "Iteration 21, loss = 0.02148833\n",
      "Iteration 22, loss = 0.02292281\n",
      "Iteration 23, loss = 0.02322481\n",
      "Iteration 24, loss = 0.02263071\n",
      "Iteration 25, loss = 0.02221918\n",
      "Iteration 26, loss = 0.02171068\n",
      "Iteration 27, loss = 0.02273698\n",
      "Iteration 28, loss = 0.02124820\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.85      0.94      0.89       961\n",
      " non-neutral       0.67      0.43      0.52       272\n",
      "\n",
      "    accuracy                           0.83      1233\n",
      "   macro avg       0.76      0.68      0.71      1233\n",
      "weighted avg       0.81      0.83      0.81      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------GradientBoostin\n",
      "accuracy_score Score on training data:\n",
      "0.8441558441558441\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8021086780210868\n",
      "f1_score  on test data:\n",
      "0.6067952028941562\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.81      0.97      0.88       961\n",
      " non-neutral       0.65      0.22      0.33       272\n",
      "\n",
      "    accuracy                           0.80      1233\n",
      "   macro avg       0.73      0.59      0.61      1233\n",
      "weighted avg       0.78      0.80      0.76      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------AdaBoostClassif\n",
      "accuracy_score Score on training data:\n",
      "0.8267045454545454\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8094079480940795\n",
      "f1_score  on test data:\n",
      "0.6304734576757532\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.82      0.97      0.89       961\n",
      " non-neutral       0.68      0.26      0.37       272\n",
      "\n",
      "    accuracy                           0.81      1233\n",
      "   macro avg       0.75      0.61      0.63      1233\n",
      "weighted avg       0.79      0.81      0.77      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------RandomForestCla\n",
      "accuracy_score Score on training data:\n",
      "0.9882305194805194\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.819140308191403\n",
      "f1_score  on test data:\n",
      "0.6794202587639167\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.84      0.95      0.89       961\n",
      " non-neutral       0.67      0.36      0.47       272\n",
      "\n",
      "    accuracy                           0.82      1233\n",
      "   macro avg       0.75      0.65      0.68      1233\n",
      "weighted avg       0.80      0.82      0.80      1233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RANDOM SPLIT\n",
    "\n",
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "#LABE4L FEATURE\n",
    "\n",
    "\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "  # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, test_data = random_split(data, features, output, train_fraction, rand_seed)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "\n",
    "print(len(train_data)+len(test_data))\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "\n",
    "# BOW\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "train_data_features = vectorizer.fit_transform(train_data['lemma'])\n",
    "test_data_features = vectorizer.transform(test_data['lemma'])\n",
    "\n",
    "#SHAPE\n",
    "\n",
    "train_data_features.shape,  test_data_features.shape\n",
    "\n",
    "#FUNCTION FOR MODEL TRAIN\n",
    "\n",
    "\n",
    "def train_n_test_classifier(clf, train_features, train_labels, test_features, test_labels,data):\n",
    "    clf.fit(train_features, train_labels) # please learn patterns from the data\n",
    "\n",
    "   \n",
    "    print('-'*100+str(clf)[0:15])\n",
    "    print(\"accuracy_score Score on training data:\")\n",
    "    print(clf.score(train_features, train_labels))\n",
    "    \n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "    print(\"score on testing data:\")\n",
    "    \n",
    "    pred_y = clf.predict(test_features)\n",
    "    \n",
    "    data['predict']=pred_y\n",
    " \n",
    "    count=0\n",
    "    \n",
    "    print(\"accuracy_score Score on test data:\")\n",
    "    print(accuracy_score(test_labels, pred_y))\n",
    "    \n",
    "    print(\"f1_score  on test data:\")\n",
    "    print(f1_score(test_labels, pred_y, average='macro'))\n",
    "    filename='result_'+str(clf)[0:15]+'.xlsx'\n",
    "    data.to_excel(filename)\n",
    "    \n",
    "    \n",
    "    sentiment_fit=clf.fit(train_features,train_labels)\n",
    "    y_pred=sentiment_fit.predict(test_features)\n",
    "    print(classification_report(test_labels,y_pred,target_names=('neutral','non-neutral')))\n",
    "    \n",
    "    \n",
    "\n",
    "#LOGASTIC REGRESSION\n",
    "\n",
    "\n",
    "\n",
    "logistic_reg = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(logistic_reg, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "\n",
    "# MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "train_n_test_classifier(mnb,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "# S V M\n",
    "\n",
    "svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(svm,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "# MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(mlp,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "# KNN\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "train_n_test_classifier(knn, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "#TREE\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "train_n_test_classifier(dtree, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "estimator = []\n",
    "\n",
    "\n",
    "estimator.append(('LR', LogisticRegression(solver ='lbfgs',  multi_class ='multinomial',  max_iter = 200)))\n",
    "estimator.append(('SVC', SVC(gamma ='auto', probability = True)))\n",
    "estimator.append(('DTC', DecisionTreeClassifier()))\n",
    "estimator.append(('mnb',MultinomialNB()))\n",
    "estimator.append(('mlp',MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)))\n",
    "estimator.append(('knn',KNeighborsClassifier(n_neighbors=3)))     \n",
    "\n",
    "\n",
    "vot_hard = VotingClassifier(estimators = estimator, voting ='hard')\n",
    "\n",
    "train_n_test_classifier(vot_hard, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "vot_soft = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "\n",
    "train_n_test_classifier(vot_soft, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "Gradientclf = GradientBoostingClassifier(n_estimators=100,learning_rate=1.0,max_depth=1, random_state=0)\n",
    "\n",
    "train_n_test_classifier(Gradientclf,train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "adaClf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "train_n_test_classifier(adaClf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(rf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa8993ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: label\n",
      "features: ['comment', 'lemma', 'clean']\n",
      "4928\n",
      "1233\n",
      "6161\n",
      "6161\n",
      "----------------------------------------------------------------------------------------------------LogisticRegress\n",
      "accuracy_score Score on training data:\n",
      "0.8303571428571429\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7996755879967559\n",
      "f1_score  on test data:\n",
      "0.5645868986193496\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.80      0.98      0.88       961\n",
      " non-neutral       0.73      0.15      0.24       272\n",
      "\n",
      "    accuracy                           0.80      1233\n",
      "   macro avg       0.77      0.57      0.56      1233\n",
      "weighted avg       0.79      0.80      0.74      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------MultinomialNB()\n",
      "accuracy_score Score on training data:\n",
      "0.8492288961038961\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7948094079480941\n",
      "f1_score  on test data:\n",
      "0.5157522667896111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.79      1.00      0.88       961\n",
      " non-neutral       0.88      0.08      0.15       272\n",
      "\n",
      "    accuracy                           0.79      1233\n",
      "   macro avg       0.84      0.54      0.52      1233\n",
      "weighted avg       0.81      0.79      0.72      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------SVC(kernel='lin\n",
      "accuracy_score Score on training data:\n",
      "0.971387987012987\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8345498783454988\n",
      "f1_score  on test data:\n",
      "0.7039155860666158\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.85      0.96      0.90       961\n",
      " non-neutral       0.74      0.39      0.51       272\n",
      "\n",
      "    accuracy                           0.83      1233\n",
      "   macro avg       0.79      0.67      0.70      1233\n",
      "weighted avg       0.82      0.83      0.81      1233\n",
      "\n",
      "Iteration 1, loss = 0.55431815\n",
      "Iteration 2, loss = 0.49347004\n",
      "Iteration 3, loss = 0.41471603\n",
      "Iteration 4, loss = 0.28817173\n",
      "Iteration 5, loss = 0.19150254\n",
      "Iteration 6, loss = 0.13756560\n",
      "Iteration 7, loss = 0.09799809\n",
      "Iteration 8, loss = 0.06530479\n",
      "Iteration 9, loss = 0.04579926\n",
      "Iteration 10, loss = 0.03748447\n",
      "Iteration 11, loss = 0.03173582\n",
      "Iteration 12, loss = 0.02909220\n",
      "Iteration 13, loss = 0.02784381\n",
      "Iteration 14, loss = 0.02539075\n",
      "Iteration 15, loss = 0.02372444\n",
      "Iteration 16, loss = 0.02296385\n",
      "Iteration 17, loss = 0.02200028\n",
      "Iteration 18, loss = 0.02225573\n",
      "Iteration 19, loss = 0.02140262\n",
      "Iteration 20, loss = 0.02109391\n",
      "Iteration 21, loss = 0.02099506\n",
      "Iteration 22, loss = 0.02015418\n",
      "Iteration 23, loss = 0.02102202\n",
      "Iteration 24, loss = 0.02088720\n",
      "Iteration 25, loss = 0.01956856\n",
      "Iteration 26, loss = 0.02037638\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "----------------------------------------------------------------------------------------------------MLPClassifier(h\n",
      "accuracy_score Score on training data:\n",
      "0.9890422077922078\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8175182481751825\n",
      "f1_score  on test data:\n",
      "0.7390937569652586\n",
      "Iteration 1, loss = 0.55431815\n",
      "Iteration 2, loss = 0.49347004\n",
      "Iteration 3, loss = 0.41471603\n",
      "Iteration 4, loss = 0.28817173\n",
      "Iteration 5, loss = 0.19150254\n",
      "Iteration 6, loss = 0.13756560\n",
      "Iteration 7, loss = 0.09799809\n",
      "Iteration 8, loss = 0.06530479\n",
      "Iteration 9, loss = 0.04579926\n",
      "Iteration 10, loss = 0.03748447\n",
      "Iteration 11, loss = 0.03173582\n",
      "Iteration 12, loss = 0.02909220\n",
      "Iteration 13, loss = 0.02784381\n",
      "Iteration 14, loss = 0.02539075\n",
      "Iteration 15, loss = 0.02372444\n",
      "Iteration 16, loss = 0.02296385\n",
      "Iteration 17, loss = 0.02200028\n",
      "Iteration 18, loss = 0.02225573\n",
      "Iteration 19, loss = 0.02140262\n",
      "Iteration 20, loss = 0.02109391\n",
      "Iteration 21, loss = 0.02099506\n",
      "Iteration 22, loss = 0.02015418\n",
      "Iteration 23, loss = 0.02102202\n",
      "Iteration 24, loss = 0.02088720\n",
      "Iteration 25, loss = 0.01956856\n",
      "Iteration 26, loss = 0.02037638\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.89      0.88      0.88       961\n",
      " non-neutral       0.58      0.61      0.60       272\n",
      "\n",
      "    accuracy                           0.82      1233\n",
      "   macro avg       0.74      0.74      0.74      1233\n",
      "weighted avg       0.82      0.82      0.82      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------KNeighborsClass\n",
      "accuracy_score Score on training data:\n",
      "0.8701298701298701\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.732360097323601\n",
      "f1_score  on test data:\n",
      "0.6643552889469619\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.88      0.76      0.82       961\n",
      " non-neutral       0.43      0.64      0.51       272\n",
      "\n",
      "    accuracy                           0.73      1233\n",
      "   macro avg       0.66      0.70      0.66      1233\n",
      "weighted avg       0.78      0.73      0.75      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------DecisionTreeCla\n",
      "accuracy_score Score on training data:\n",
      "0.9890422077922078\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7891321978913219\n",
      "f1_score  on test data:\n",
      "0.7267864813085068\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.91      0.81      0.86       961\n",
      " non-neutral       0.52      0.71      0.60       272\n",
      "\n",
      "    accuracy                           0.79      1233\n",
      "   macro avg       0.71      0.76      0.73      1233\n",
      "weighted avg       0.82      0.79      0.80      1233\n",
      "\n",
      "Iteration 1, loss = 0.55431815\n",
      "Iteration 2, loss = 0.49347004\n",
      "Iteration 3, loss = 0.41471603\n",
      "Iteration 4, loss = 0.28817173\n",
      "Iteration 5, loss = 0.19150254\n",
      "Iteration 6, loss = 0.13756560\n",
      "Iteration 7, loss = 0.09799809\n",
      "Iteration 8, loss = 0.06530479\n",
      "Iteration 9, loss = 0.04579926\n",
      "Iteration 10, loss = 0.03748447\n",
      "Iteration 11, loss = 0.03173582\n",
      "Iteration 12, loss = 0.02909220\n",
      "Iteration 13, loss = 0.02784381\n",
      "Iteration 14, loss = 0.02539075\n",
      "Iteration 15, loss = 0.02372444\n",
      "Iteration 16, loss = 0.02296385\n",
      "Iteration 17, loss = 0.02200028\n",
      "Iteration 18, loss = 0.02225573\n",
      "Iteration 19, loss = 0.02140262\n",
      "Iteration 20, loss = 0.02109391\n",
      "Iteration 21, loss = 0.02099506\n",
      "Iteration 22, loss = 0.02015418\n",
      "Iteration 23, loss = 0.02102202\n",
      "Iteration 24, loss = 0.02088720\n",
      "Iteration 25, loss = 0.01956856\n",
      "Iteration 26, loss = 0.02037638\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "----------------------------------------------------------------------------------------------------VotingClassifie\n",
      "accuracy_score Score on training data:\n",
      "0.90625\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8126520681265207\n",
      "f1_score  on test data:\n",
      "0.6144961039910237\n",
      "Iteration 1, loss = 0.55431815\n",
      "Iteration 2, loss = 0.49347004\n",
      "Iteration 3, loss = 0.41471603\n",
      "Iteration 4, loss = 0.28817173\n",
      "Iteration 5, loss = 0.19150254\n",
      "Iteration 6, loss = 0.13756560\n",
      "Iteration 7, loss = 0.09799809\n",
      "Iteration 8, loss = 0.06530479\n",
      "Iteration 9, loss = 0.04579926\n",
      "Iteration 10, loss = 0.03748447\n",
      "Iteration 11, loss = 0.03173582\n",
      "Iteration 12, loss = 0.02909220\n",
      "Iteration 13, loss = 0.02784381\n",
      "Iteration 14, loss = 0.02539075\n",
      "Iteration 15, loss = 0.02372444\n",
      "Iteration 16, loss = 0.02296385\n",
      "Iteration 17, loss = 0.02200028\n",
      "Iteration 18, loss = 0.02225573\n",
      "Iteration 19, loss = 0.02140262\n",
      "Iteration 20, loss = 0.02109391\n",
      "Iteration 21, loss = 0.02099506\n",
      "Iteration 22, loss = 0.02015418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23, loss = 0.02102202\n",
      "Iteration 24, loss = 0.02088720\n",
      "Iteration 25, loss = 0.01956856\n",
      "Iteration 26, loss = 0.02037638\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.82      0.98      0.89       961\n",
      " non-neutral       0.78      0.22      0.34       272\n",
      "\n",
      "    accuracy                           0.81      1233\n",
      "   macro avg       0.80      0.60      0.62      1233\n",
      "weighted avg       0.81      0.81      0.77      1233\n",
      "\n",
      "Iteration 1, loss = 0.55431815\n",
      "Iteration 2, loss = 0.49347004\n",
      "Iteration 3, loss = 0.41471603\n",
      "Iteration 4, loss = 0.28817173\n",
      "Iteration 5, loss = 0.19150254\n",
      "Iteration 6, loss = 0.13756560\n",
      "Iteration 7, loss = 0.09799809\n",
      "Iteration 8, loss = 0.06530479\n",
      "Iteration 9, loss = 0.04579926\n",
      "Iteration 10, loss = 0.03748447\n",
      "Iteration 11, loss = 0.03173582\n",
      "Iteration 12, loss = 0.02909220\n",
      "Iteration 13, loss = 0.02784381\n",
      "Iteration 14, loss = 0.02539075\n",
      "Iteration 15, loss = 0.02372444\n",
      "Iteration 16, loss = 0.02296385\n",
      "Iteration 17, loss = 0.02200028\n",
      "Iteration 18, loss = 0.02225573\n",
      "Iteration 19, loss = 0.02140262\n",
      "Iteration 20, loss = 0.02109391\n",
      "Iteration 21, loss = 0.02099506\n",
      "Iteration 22, loss = 0.02015418\n",
      "Iteration 23, loss = 0.02102202\n",
      "Iteration 24, loss = 0.02088720\n",
      "Iteration 25, loss = 0.01956856\n",
      "Iteration 26, loss = 0.02037638\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "----------------------------------------------------------------------------------------------------VotingClassifie\n",
      "accuracy_score Score on training data:\n",
      "0.987012987012987\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8369829683698297\n",
      "f1_score  on test data:\n",
      "0.712138406053813\n",
      "Iteration 1, loss = 0.55431815\n",
      "Iteration 2, loss = 0.49347004\n",
      "Iteration 3, loss = 0.41471603\n",
      "Iteration 4, loss = 0.28817173\n",
      "Iteration 5, loss = 0.19150254\n",
      "Iteration 6, loss = 0.13756560\n",
      "Iteration 7, loss = 0.09799809\n",
      "Iteration 8, loss = 0.06530479\n",
      "Iteration 9, loss = 0.04579926\n",
      "Iteration 10, loss = 0.03748447\n",
      "Iteration 11, loss = 0.03173582\n",
      "Iteration 12, loss = 0.02909220\n",
      "Iteration 13, loss = 0.02784381\n",
      "Iteration 14, loss = 0.02539075\n",
      "Iteration 15, loss = 0.02372444\n",
      "Iteration 16, loss = 0.02296385\n",
      "Iteration 17, loss = 0.02200028\n",
      "Iteration 18, loss = 0.02225573\n",
      "Iteration 19, loss = 0.02140262\n",
      "Iteration 20, loss = 0.02109391\n",
      "Iteration 21, loss = 0.02099506\n",
      "Iteration 22, loss = 0.02015418\n",
      "Iteration 23, loss = 0.02102202\n",
      "Iteration 24, loss = 0.02088720\n",
      "Iteration 25, loss = 0.01956856\n",
      "Iteration 26, loss = 0.02037638\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.85      0.96      0.90       961\n",
      " non-neutral       0.73      0.40      0.52       272\n",
      "\n",
      "    accuracy                           0.84      1233\n",
      "   macro avg       0.79      0.68      0.71      1233\n",
      "weighted avg       0.82      0.84      0.82      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------GradientBoostin\n",
      "accuracy_score Score on training data:\n",
      "0.8650568181818182\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7883211678832117\n",
      "f1_score  on test data:\n",
      "0.6494843227358724\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.83      0.91      0.87       961\n",
      " non-neutral       0.53      0.36      0.43       272\n",
      "\n",
      "    accuracy                           0.79      1233\n",
      "   macro avg       0.68      0.63      0.65      1233\n",
      "weighted avg       0.77      0.79      0.77      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------AdaBoostClassif\n",
      "accuracy_score Score on training data:\n",
      "0.8423295454545454\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7988645579886455\n",
      "f1_score  on test data:\n",
      "0.6238856198216991\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.82      0.95      0.88       961\n",
      " non-neutral       0.60      0.26      0.37       272\n",
      "\n",
      "    accuracy                           0.80      1233\n",
      "   macro avg       0.71      0.61      0.62      1233\n",
      "weighted avg       0.77      0.80      0.77      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------RandomForestCla\n",
      "accuracy_score Score on training data:\n",
      "0.9886363636363636\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8085969180859692\n",
      "f1_score  on test data:\n",
      "0.72598967946062\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.88      0.87      0.88       961\n",
      " non-neutral       0.56      0.59      0.58       272\n",
      "\n",
      "    accuracy                           0.81      1233\n",
      "   macro avg       0.72      0.73      0.73      1233\n",
      "weighted avg       0.81      0.81      0.81      1233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RANDOM SPLIT\n",
    "\n",
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "#LABE4L FEATURE\n",
    "\n",
    "\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "  # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, test_data = random_split(data, features, output, train_fraction, rand_seed)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "\n",
    "print(len(train_data)+len(test_data))\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "\n",
    "# TFIDF\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True, max_df=0.5, stop_words=None, use_idf=True)\n",
    "train_data_features = vectorizer.fit_transform(train_data['clean'])\n",
    "test_data_features = vectorizer.transform(test_data['clean'])\n",
    "\n",
    "#SHAPE\n",
    "\n",
    "train_data_features.shape,  test_data_features.shape\n",
    "\n",
    "#FUNCTION FOR MODEL TRAIN\n",
    "\n",
    "\n",
    "def train_n_test_classifier(clf, train_features, train_labels, test_features, test_labels,data):\n",
    "    clf.fit(train_features, train_labels) # please learn patterns from the data\n",
    "\n",
    "   \n",
    "    print('-'*100+str(clf)[0:15])\n",
    "    print(\"accuracy_score Score on training data:\")\n",
    "    print(clf.score(train_features, train_labels))\n",
    "    \n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "    print(\"score on testing data:\")\n",
    "    \n",
    "    pred_y = clf.predict(test_features)\n",
    "    \n",
    "    data['predict']=pred_y\n",
    " \n",
    "    count=0\n",
    "    \n",
    "    print(\"accuracy_score Score on test data:\")\n",
    "    print(accuracy_score(test_labels, pred_y))\n",
    "    \n",
    "    print(\"f1_score  on test data:\")\n",
    "    print(f1_score(test_labels, pred_y, average='macro'))\n",
    "    filename='result_'+str(clf)[0:15]+'.xlsx'\n",
    "    data.to_excel(filename)\n",
    "    \n",
    "    \n",
    "    sentiment_fit=clf.fit(train_features,train_labels)\n",
    "    y_pred=sentiment_fit.predict(test_features)\n",
    "    print(classification_report(test_labels,y_pred,target_names=('neutral','non-neutral')))\n",
    "    \n",
    "    \n",
    "\n",
    "#LOGASTIC REGRESSION\n",
    "\n",
    "\n",
    "\n",
    "logistic_reg = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(logistic_reg, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "\n",
    "# MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "train_n_test_classifier(mnb,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "# S V M\n",
    "\n",
    "svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(svm,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "# MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(mlp,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "# KNN\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "train_n_test_classifier(knn, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "#TREE\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "train_n_test_classifier(dtree, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "estimator = []\n",
    "\n",
    "\n",
    "estimator.append(('LR', LogisticRegression(solver ='lbfgs',  multi_class ='multinomial',  max_iter = 200)))\n",
    "estimator.append(('SVC', SVC(gamma ='auto', probability = True)))\n",
    "estimator.append(('DTC', DecisionTreeClassifier()))\n",
    "estimator.append(('mnb',MultinomialNB()))\n",
    "estimator.append(('mlp',MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)))\n",
    "estimator.append(('knn',KNeighborsClassifier(n_neighbors=3)))     \n",
    "\n",
    "\n",
    "vot_hard = VotingClassifier(estimators = estimator, voting ='hard')\n",
    "\n",
    "train_n_test_classifier(vot_hard, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "vot_soft = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "\n",
    "train_n_test_classifier(vot_soft, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "Gradientclf = GradientBoostingClassifier(n_estimators=100,learning_rate=1.0,max_depth=1, random_state=0)\n",
    "\n",
    "train_n_test_classifier(Gradientclf,train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "adaClf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "train_n_test_classifier(adaClf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(rf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fdd5629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: label\n",
      "features: ['comment', 'lemma', 'clean']\n",
      "4928\n",
      "1233\n",
      "6161\n",
      "6161\n",
      "----------------------------------------------------------------------------------------------------LogisticRegress\n",
      "accuracy_score Score on training data:\n",
      "0.9675324675324676\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8256285482562855\n",
      "f1_score  on test data:\n",
      "0.6824087551889589\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.84      0.96      0.90       961\n",
      " non-neutral       0.71      0.35      0.47       272\n",
      "\n",
      "    accuracy                           0.83      1233\n",
      "   macro avg       0.78      0.65      0.68      1233\n",
      "weighted avg       0.81      0.83      0.80      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------MultinomialNB()\n",
      "accuracy_score Score on training data:\n",
      "0.9571834415584416\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8199513381995134\n",
      "f1_score  on test data:\n",
      "0.643864168618267\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.82      0.98      0.89       961\n",
      " non-neutral       0.77      0.26      0.39       272\n",
      "\n",
      "    accuracy                           0.82      1233\n",
      "   macro avg       0.80      0.62      0.64      1233\n",
      "weighted avg       0.81      0.82      0.78      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------SVC(kernel='lin\n",
      "accuracy_score Score on training data:\n",
      "0.9857954545454546\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8215733982157339\n",
      "f1_score  on test data:\n",
      "0.7070145423438837\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.86      0.93      0.89       961\n",
      " non-neutral       0.64      0.44      0.52       272\n",
      "\n",
      "    accuracy                           0.82      1233\n",
      "   macro avg       0.75      0.69      0.71      1233\n",
      "weighted avg       0.81      0.82      0.81      1233\n",
      "\n",
      "Iteration 1, loss = 0.53881679\n",
      "Iteration 2, loss = 0.42653447\n",
      "Iteration 3, loss = 0.30291878\n",
      "Iteration 4, loss = 0.19369236\n",
      "Iteration 5, loss = 0.13221944\n",
      "Iteration 6, loss = 0.08342243\n",
      "Iteration 7, loss = 0.05269765\n",
      "Iteration 8, loss = 0.03883665\n",
      "Iteration 9, loss = 0.03351001\n",
      "Iteration 10, loss = 0.03303654\n",
      "Iteration 11, loss = 0.02785031\n",
      "Iteration 12, loss = 0.02668035\n",
      "Iteration 13, loss = 0.02475242\n",
      "Iteration 14, loss = 0.02308797\n",
      "Iteration 15, loss = 0.02263198\n",
      "Iteration 16, loss = 0.02173890\n",
      "Iteration 17, loss = 0.02079299\n",
      "Iteration 18, loss = 0.02148706\n",
      "Iteration 19, loss = 0.02090416\n",
      "Iteration 20, loss = 0.02127964\n",
      "Iteration 21, loss = 0.02034860\n",
      "Iteration 22, loss = 0.01987572\n",
      "Iteration 23, loss = 0.02007126\n",
      "Iteration 24, loss = 0.01966501\n",
      "Iteration 25, loss = 0.01911363\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "----------------------------------------------------------------------------------------------------MLPClassifier(h\n",
      "accuracy_score Score on training data:\n",
      "0.9890422077922078\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7785888077858881\n",
      "f1_score  on test data:\n",
      "0.7178714362943748\n",
      "Iteration 1, loss = 0.53881679\n",
      "Iteration 2, loss = 0.42653447\n",
      "Iteration 3, loss = 0.30291878\n",
      "Iteration 4, loss = 0.19369236\n",
      "Iteration 5, loss = 0.13221944\n",
      "Iteration 6, loss = 0.08342243\n",
      "Iteration 7, loss = 0.05269765\n",
      "Iteration 8, loss = 0.03883665\n",
      "Iteration 9, loss = 0.03351001\n",
      "Iteration 10, loss = 0.03303654\n",
      "Iteration 11, loss = 0.02785031\n",
      "Iteration 12, loss = 0.02668035\n",
      "Iteration 13, loss = 0.02475242\n",
      "Iteration 14, loss = 0.02308797\n",
      "Iteration 15, loss = 0.02263198\n",
      "Iteration 16, loss = 0.02173890\n",
      "Iteration 17, loss = 0.02079299\n",
      "Iteration 18, loss = 0.02148706\n",
      "Iteration 19, loss = 0.02090416\n",
      "Iteration 20, loss = 0.02127964\n",
      "Iteration 21, loss = 0.02034860\n",
      "Iteration 22, loss = 0.01987572\n",
      "Iteration 23, loss = 0.02007126\n",
      "Iteration 24, loss = 0.01966501\n",
      "Iteration 25, loss = 0.01911363\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.91      0.80      0.85       961\n",
      " non-neutral       0.50      0.71      0.59       272\n",
      "\n",
      "    accuracy                           0.78      1233\n",
      "   macro avg       0.70      0.76      0.72      1233\n",
      "weighted avg       0.82      0.78      0.79      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------KNeighborsClass\n",
      "accuracy_score Score on training data:\n",
      "0.6897321428571429\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.5725871857258719\n",
      "f1_score  on test data:\n",
      "0.5554628948673657\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.92      0.49      0.64       961\n",
      " non-neutral       0.32      0.85      0.47       272\n",
      "\n",
      "    accuracy                           0.57      1233\n",
      "   macro avg       0.62      0.67      0.56      1233\n",
      "weighted avg       0.79      0.57      0.60      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------DecisionTreeCla\n",
      "accuracy_score Score on training data:\n",
      "0.9890422077922078\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.772911597729116\n",
      "f1_score  on test data:\n",
      "0.6943870035692029\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.88      0.82      0.85       961\n",
      " non-neutral       0.49      0.60      0.54       272\n",
      "\n",
      "    accuracy                           0.77      1233\n",
      "   macro avg       0.68      0.71      0.69      1233\n",
      "weighted avg       0.79      0.77      0.78      1233\n",
      "\n",
      "Iteration 1, loss = 0.53881679\n",
      "Iteration 2, loss = 0.42653447\n",
      "Iteration 3, loss = 0.30291878\n",
      "Iteration 4, loss = 0.19369236\n",
      "Iteration 5, loss = 0.13221944\n",
      "Iteration 6, loss = 0.08342243\n",
      "Iteration 7, loss = 0.05269765\n",
      "Iteration 8, loss = 0.03883665\n",
      "Iteration 9, loss = 0.03351001\n",
      "Iteration 10, loss = 0.03303654\n",
      "Iteration 11, loss = 0.02785031\n",
      "Iteration 12, loss = 0.02668035\n",
      "Iteration 13, loss = 0.02475242\n",
      "Iteration 14, loss = 0.02308797\n",
      "Iteration 15, loss = 0.02263198\n",
      "Iteration 16, loss = 0.02173890\n",
      "Iteration 17, loss = 0.02079299\n",
      "Iteration 18, loss = 0.02148706\n",
      "Iteration 19, loss = 0.02090416\n",
      "Iteration 20, loss = 0.02127964\n",
      "Iteration 21, loss = 0.02034860\n",
      "Iteration 22, loss = 0.01987572\n",
      "Iteration 23, loss = 0.02007126\n",
      "Iteration 24, loss = 0.01966501\n",
      "Iteration 25, loss = 0.01911363\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "----------------------------------------------------------------------------------------------------VotingClassifie\n",
      "accuracy_score Score on training data:\n",
      "0.9799107142857143\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8288726682887266\n",
      "f1_score  on test data:\n",
      "0.6895477755118358\n",
      "Iteration 1, loss = 0.53881679\n",
      "Iteration 2, loss = 0.42653447\n",
      "Iteration 3, loss = 0.30291878\n",
      "Iteration 4, loss = 0.19369236\n",
      "Iteration 5, loss = 0.13221944\n",
      "Iteration 6, loss = 0.08342243\n",
      "Iteration 7, loss = 0.05269765\n",
      "Iteration 8, loss = 0.03883665\n",
      "Iteration 9, loss = 0.03351001\n",
      "Iteration 10, loss = 0.03303654\n",
      "Iteration 11, loss = 0.02785031\n",
      "Iteration 12, loss = 0.02668035\n",
      "Iteration 13, loss = 0.02475242\n",
      "Iteration 14, loss = 0.02308797\n",
      "Iteration 15, loss = 0.02263198\n",
      "Iteration 16, loss = 0.02173890\n",
      "Iteration 17, loss = 0.02079299\n",
      "Iteration 18, loss = 0.02148706\n",
      "Iteration 19, loss = 0.02090416\n",
      "Iteration 20, loss = 0.02127964\n",
      "Iteration 21, loss = 0.02034860\n",
      "Iteration 22, loss = 0.01987572\n",
      "Iteration 23, loss = 0.02007126\n",
      "Iteration 24, loss = 0.01966501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25, loss = 0.01911363\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.84      0.96      0.90       961\n",
      " non-neutral       0.72      0.36      0.48       272\n",
      "\n",
      "    accuracy                           0.83      1233\n",
      "   macro avg       0.78      0.66      0.69      1233\n",
      "weighted avg       0.81      0.83      0.81      1233\n",
      "\n",
      "Iteration 1, loss = 0.53881679\n",
      "Iteration 2, loss = 0.42653447\n",
      "Iteration 3, loss = 0.30291878\n",
      "Iteration 4, loss = 0.19369236\n",
      "Iteration 5, loss = 0.13221944\n",
      "Iteration 6, loss = 0.08342243\n",
      "Iteration 7, loss = 0.05269765\n",
      "Iteration 8, loss = 0.03883665\n",
      "Iteration 9, loss = 0.03351001\n",
      "Iteration 10, loss = 0.03303654\n",
      "Iteration 11, loss = 0.02785031\n",
      "Iteration 12, loss = 0.02668035\n",
      "Iteration 13, loss = 0.02475242\n",
      "Iteration 14, loss = 0.02308797\n",
      "Iteration 15, loss = 0.02263198\n",
      "Iteration 16, loss = 0.02173890\n",
      "Iteration 17, loss = 0.02079299\n",
      "Iteration 18, loss = 0.02148706\n",
      "Iteration 19, loss = 0.02090416\n",
      "Iteration 20, loss = 0.02127964\n",
      "Iteration 21, loss = 0.02034860\n",
      "Iteration 22, loss = 0.01987572\n",
      "Iteration 23, loss = 0.02007126\n",
      "Iteration 24, loss = 0.01966501\n",
      "Iteration 25, loss = 0.01911363\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "----------------------------------------------------------------------------------------------------VotingClassifie\n",
      "accuracy_score Score on training data:\n",
      "0.9874188311688312\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8240064882400648\n",
      "f1_score  on test data:\n",
      "0.7306106545121741\n",
      "Iteration 1, loss = 0.53881679\n",
      "Iteration 2, loss = 0.42653447\n",
      "Iteration 3, loss = 0.30291878\n",
      "Iteration 4, loss = 0.19369236\n",
      "Iteration 5, loss = 0.13221944\n",
      "Iteration 6, loss = 0.08342243\n",
      "Iteration 7, loss = 0.05269765\n",
      "Iteration 8, loss = 0.03883665\n",
      "Iteration 9, loss = 0.03351001\n",
      "Iteration 10, loss = 0.03303654\n",
      "Iteration 11, loss = 0.02785031\n",
      "Iteration 12, loss = 0.02668035\n",
      "Iteration 13, loss = 0.02475242\n",
      "Iteration 14, loss = 0.02308797\n",
      "Iteration 15, loss = 0.02263198\n",
      "Iteration 16, loss = 0.02173890\n",
      "Iteration 17, loss = 0.02079299\n",
      "Iteration 18, loss = 0.02148706\n",
      "Iteration 19, loss = 0.02090416\n",
      "Iteration 20, loss = 0.02127964\n",
      "Iteration 21, loss = 0.02034860\n",
      "Iteration 22, loss = 0.01987572\n",
      "Iteration 23, loss = 0.02007126\n",
      "Iteration 24, loss = 0.01966501\n",
      "Iteration 25, loss = 0.01911363\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.88      0.91      0.89       961\n",
      " non-neutral       0.62      0.55      0.58       272\n",
      "\n",
      "    accuracy                           0.83      1233\n",
      "   macro avg       0.75      0.73      0.74      1233\n",
      "weighted avg       0.82      0.83      0.82      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------GradientBoostin\n",
      "accuracy_score Score on training data:\n",
      "0.8494318181818182\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7964314679643146\n",
      "f1_score  on test data:\n",
      "0.6000441978668714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.81      0.96      0.88       961\n",
      " non-neutral       0.61      0.22      0.32       272\n",
      "\n",
      "    accuracy                           0.80      1233\n",
      "   macro avg       0.71      0.59      0.60      1233\n",
      "weighted avg       0.77      0.80      0.76      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------AdaBoostClassif\n",
      "accuracy_score Score on training data:\n",
      "0.8265016233766234\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8094079480940795\n",
      "f1_score  on test data:\n",
      "0.6096866096866097\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.81      0.98      0.89       961\n",
      " non-neutral       0.73      0.21      0.33       272\n",
      "\n",
      "    accuracy                           0.81      1233\n",
      "   macro avg       0.77      0.60      0.61      1233\n",
      "weighted avg       0.80      0.81      0.77      1233\n",
      "\n",
      "----------------------------------------------------------------------------------------------------RandomForestCla\n",
      "accuracy_score Score on training data:\n",
      "0.9886363636363636\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7915652879156528\n",
      "f1_score  on test data:\n",
      "0.707146190777648\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.88      0.85      0.86       961\n",
      " non-neutral       0.53      0.58      0.55       272\n",
      "\n",
      "    accuracy                           0.79      1233\n",
      "   macro avg       0.70      0.71      0.71      1233\n",
      "weighted avg       0.80      0.79      0.80      1233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RANDOM SPLIT\n",
    "\n",
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "#LABE4L FEATURE\n",
    "\n",
    "\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "  # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, test_data = random_split(data, features, output, train_fraction, rand_seed)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "\n",
    "print(len(train_data)+len(test_data))\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "\n",
    "# BOW\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "train_data_features = vectorizer.fit_transform(train_data['clean'])\n",
    "test_data_features = vectorizer.transform(test_data['clean'])\n",
    "\n",
    "#SHAPE\n",
    "\n",
    "train_data_features.shape,  test_data_features.shape\n",
    "\n",
    "#FUNCTION FOR MODEL TRAIN\n",
    "\n",
    "\n",
    "def train_n_test_classifier(clf, train_features, train_labels, test_features, test_labels,data):\n",
    "    clf.fit(train_features, train_labels) # please learn patterns from the data\n",
    "\n",
    "   \n",
    "    print('-'*100+str(clf)[0:15])\n",
    "    print(\"accuracy_score Score on training data:\")\n",
    "    print(clf.score(train_features, train_labels))\n",
    "    \n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "    print(\"score on testing data:\")\n",
    "    \n",
    "    pred_y = clf.predict(test_features)\n",
    "    \n",
    "    data['predict']=pred_y\n",
    " \n",
    "    count=0\n",
    "    \n",
    "    print(\"accuracy_score Score on test data:\")\n",
    "    print(accuracy_score(test_labels, pred_y))\n",
    "    \n",
    "    print(\"f1_score  on test data:\")\n",
    "    print(f1_score(test_labels, pred_y, average='macro'))\n",
    "    filename='result_'+str(clf)[0:15]+'.xlsx'\n",
    "    data.to_excel(filename)\n",
    "    \n",
    "    \n",
    "    sentiment_fit=clf.fit(train_features,train_labels)\n",
    "    y_pred=sentiment_fit.predict(test_features)\n",
    "    print(classification_report(test_labels,y_pred,target_names=('neutral','non-neutral')))\n",
    "    \n",
    "    \n",
    "\n",
    "#LOGASTIC REGRESSION\n",
    "\n",
    "\n",
    "\n",
    "logistic_reg = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(logistic_reg, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "\n",
    "# MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "train_n_test_classifier(mnb,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "# S V M\n",
    "\n",
    "svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(svm,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "# MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(mlp,  train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "# KNN\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "train_n_test_classifier(knn, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "#TREE\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "train_n_test_classifier(dtree, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "estimator = []\n",
    "\n",
    "\n",
    "estimator.append(('LR', LogisticRegression(solver ='lbfgs',  multi_class ='multinomial',  max_iter = 200)))\n",
    "estimator.append(('SVC', SVC(gamma ='auto', probability = True)))\n",
    "estimator.append(('DTC', DecisionTreeClassifier()))\n",
    "estimator.append(('mnb',MultinomialNB()))\n",
    "estimator.append(('mlp',MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)))\n",
    "estimator.append(('knn',KNeighborsClassifier(n_neighbors=3)))     \n",
    "\n",
    "\n",
    "vot_hard = VotingClassifier(estimators = estimator, voting ='hard')\n",
    "\n",
    "train_n_test_classifier(vot_hard, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "vot_soft = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "\n",
    "train_n_test_classifier(vot_soft, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "\n",
    "Gradientclf = GradientBoostingClassifier(n_estimators=100,learning_rate=1.0,max_depth=1, random_state=0)\n",
    "\n",
    "train_n_test_classifier(Gradientclf,train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "adaClf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "train_n_test_classifier(adaClf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(rf, train_data_features, train_data[output],\n",
    "                        test_data_features, test_data[output],test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2700d4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2ef84f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
