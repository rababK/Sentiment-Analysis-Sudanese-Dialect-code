{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a7ce4a1",
   "metadata": {},
   "source": [
    "#  preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "052b6e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "from aranorm import normalize_arabic_text\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import stanza\n",
    "from aranorm import normalize_arabic_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99863b1",
   "metadata": {},
   "source": [
    "#  Normalization using ARANORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83e04d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normlizeComment(file1,file2):\n",
    "\n",
    "    with open(file1, 'r',encoding=\"utf8\") as file1:\n",
    "        with open(file2, 'w',newline='',encoding=\"utf8\") as file2:\n",
    "            for row in file1:\n",
    "       \n",
    "       \n",
    "                file2.write(normalize_arabic_text(row))\n",
    "                file2.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "normlizeComment('data/ALLComments.txt','data/NormlizeComment')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e91d343",
   "metadata": {},
   "source": [
    "# Remove stopwords  using nltk (\"except 'ما' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7546683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rabab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "def removeStopWords(file1,file2):\n",
    "\n",
    "    arabicStopWords= stopwords.words(\"arabic\")\n",
    "    arabicFilteredStopWordList=[i for i in arabicStopWords if i !=\"ما\"]\n",
    "\n",
    "    with open(file1, 'r',encoding=\"utf8\") as file1:\n",
    "        with open(file2, 'w',newline='',encoding=\"utf8\") as file2:\n",
    "\n",
    "            for row in file1:\n",
    "                tokenizedRow = word_tokenize(row)\n",
    "                commentWithNoStopWords= ' '.join([i for i in tokenizedRow if i not in arabicFilteredStopWordList])\n",
    "                file2.write(commentWithNoStopWords)\n",
    "                file2.write('\\n')\n",
    "removeStopWords('data/NormlizeComment','data/commentsWithNoStopWords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138a1a8e",
   "metadata": {},
   "source": [
    "# Handling negotation by join \"ما\" with next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3dfda58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handlingNegotation(file1,file2):\n",
    "     with open(file1, 'r',encoding=\"utf8\") as file1:\n",
    "        with open(file2, 'w',newline='',encoding=\"utf8\") as file2:\n",
    "            \n",
    "            for row in file1:\n",
    "                \n",
    "                commenAslist=row.split()\n",
    "                comment=row\n",
    "                if \"ما\" in commenAslist:\n",
    "                    \n",
    "                    i=commenAslist.index(\"ما\")\n",
    "                    if i<len(commenAslist)-1:\n",
    "                        comment=\"\"\n",
    "                        commenAslist[i]=commenAslist[i]+commenAslist[i+1]\n",
    "                        commenAslist.pop(i+1)\n",
    "                        for w in commenAslist:\n",
    "                            comment=comment+w+' '\n",
    "                        comment=comment+'\\n'\n",
    "                         \n",
    "                          \n",
    "                file2.write(comment)\n",
    "                \n",
    "    \n",
    "\n",
    "handlingNegotation('data/commentsWithNoStopWords','data/commentsAfterHandlingNegotation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0218b09c",
   "metadata": {},
   "source": [
    "# Tokenization using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "786d2d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/commentsAfterHandlingNegotation\", 'r',encoding=\"utf8\") as file1:\n",
    "    with open(\"data/commentsWithtokenization\", 'w',newline='',encoding=\"utf8\") as file2:\n",
    "        for row in file1:\n",
    "            tokenlist=[word for word in word_tokenize(row)]\n",
    "            file2.write(str(tokenlist))\n",
    "            file2.write('\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe30e39",
   "metadata": {},
   "source": [
    "# Remove duplicated word and characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09143d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "\n",
    "def remove_duplicated_characters(text):\n",
    "    result=[]\n",
    "    text=''.join(i for i,_ in itertools.groupby(text))\n",
    "    result.append(text)\n",
    "    return ''.join(result)\n",
    "    \n",
    "\n",
    "def remove_duplicated_words(text):\n",
    "    s1=[]\n",
    "    text=' '.join(k for k,v in itertools.groupby(text.replace(\"&lt;/Sent&gt;\",\"\").split()))\n",
    "    s=re.sub(r'b(.+)(\\s+\\b)+',r'\\1',text)\n",
    "    return ''.join(s)\n",
    "    \n",
    "    \n",
    "with open(\"data/commentsAfterHandlingNegotation\", 'r',encoding=\"utf8\") as file1:\n",
    "    with open(\"data/commentsWithNoDuplicated\", 'w',newline='',encoding=\"utf8\") as file2:\n",
    "        for row in file1:\n",
    "            phace1=remove_duplicated_characters(row)\n",
    "            phace2=remove_duplicated_words(phace1)\n",
    "            file2.write(phace2)\n",
    "            file2.write('\\n')\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ac84add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 12:26:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765409a0208d4b94bed62cede6091ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 12:26:26 WARNING: Language ar package default expects mwt, which has been added\n",
      "2023-02-18 12:26:27 INFO: Loading these models for language: ar (Arabic):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | padt    |\n",
      "| mwt       | padt    |\n",
      "| lemma     | padt    |\n",
      "=======================\n",
      "\n",
      "2023-02-18 12:26:27 INFO: Use device: cpu\n",
      "2023-02-18 12:26:27 INFO: Loading: tokenize\n",
      "2023-02-18 12:26:30 INFO: Loading: mwt\n",
      "2023-02-18 12:26:30 INFO: Loading: lemma\n",
      "2023-02-18 12:26:30 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='ar', processors='tokenize,lemma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26747205",
   "metadata": {},
   "source": [
    "# Lemmatization using stanza "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8dcbde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/commentsAfterHandlingNegotation\", 'r',encoding=\"utf8\") as file1:\n",
    "    with open(\"data/commentsLemmatization\", 'w',newline='',encoding=\"utf8\") as file2:\n",
    "        for row in file1:\n",
    "            doc = nlp(row)\n",
    "            lema=''.join(word.lemma+' ' for sent in doc.sentences for word in sent.words)\n",
    "        \n",
    "            file2.write(lema)\n",
    "            file2.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67e692a",
   "metadata": {},
   "source": [
    "# Normalization to remove tashkeel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0a9da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "normlizeComment('data/commentsLemmatization','data/NormlizeCommentslemma')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
